# üîÑ COMPLETE CI/CD PIPELINE - GITHUB ACTIONS (END-TO-END)

Let me show you **EXACTLY** how CI/CD is done in real companies for Data Engineering + ML projects using GitHub Actions!

---

## üìã **JIRA STORY: CPG-701**

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  CPG-701: Implement CI/CD Pipeline with GitHub Actions       ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                               ‚ïë
‚ïë  As a: Data Engineering Team                                  ‚ïë
‚ïë  I want to: Automate testing, deployment & infrastructure    ‚ïë
‚ïë  So that: We deploy safely, faster, and with fewer errors    ‚ïë
‚ïë                                                               ‚ïë
‚ïë  Acceptance Criteria:                                         ‚ïë
‚ïë  ‚úì Set up GitHub repository structure                        ‚ïë
‚ïë  ‚úì Implement automated testing (unit, integration)           ‚ïë
‚ïë  ‚úì Create GitHub Actions workflows                           ‚ïë
‚ïë  ‚úì Deploy to dev/staging/prod environments                   ‚ïë
‚ïë  ‚úì Infrastructure as Code (Terraform)                        ‚ïë
‚ïë  ‚úì Data quality checks in pipeline                           ‚ïë
‚ïë  ‚úì ML model validation before deployment                     ‚ïë
‚ïë  ‚úì Automated rollback on failure                             ‚ïë
‚ïë  ‚úì Slack notifications on deploy status                      ‚ïë
‚ïë                                                               ‚ïë
‚ïë  Story Points: 8                                              ‚ïë
‚ïë  Sprint: 18                                                   ‚ïë
‚ïë  Assignee: You + David (DevOps)                              ‚ïë
‚ïë                                                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

---

## üèóÔ∏è **COMPLETE CI/CD ARCHITECTURE**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DEVELOPER WORKFLOW                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  Developer (You)                                                ‚îÇ
‚îÇ  ‚îú‚îÄ Write code (Glue jobs, Lambda, dbt, Python)                ‚îÇ
‚îÇ  ‚îú‚îÄ Write tests (pytest, Great Expectations)                   ‚îÇ
‚îÇ  ‚îú‚îÄ Commit to feature branch                                   ‚îÇ
‚îÇ  ‚îî‚îÄ Create Pull Request                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              GITHUB ACTIONS - CI PIPELINE                       ‚îÇ
‚îÇ              (Triggered on Pull Request)                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  Step 1: Code Quality Checks                                    ‚îÇ
‚îÇ  ‚îú‚îÄ Linting (pylint, flake8, black)                            ‚îÇ
‚îÇ  ‚îú‚îÄ Security scanning (Bandit, Safety)                         ‚îÇ
‚îÇ  ‚îú‚îÄ SQL validation (sqlfluff)                                  ‚îÇ
‚îÇ  ‚îî‚îÄ Documentation check                                         ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Step 2: Unit Tests                                             ‚îÇ
‚îÇ  ‚îú‚îÄ pytest (Python unit tests)                                 ‚îÇ
‚îÇ  ‚îú‚îÄ dbt test (SQL data tests)                                  ‚îÇ
‚îÇ  ‚îú‚îÄ Coverage report (>80% required)                            ‚îÇ
‚îÇ  ‚îî‚îÄ Upload to CodeCov                                           ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Step 3: Integration Tests                                      ‚îÇ
‚îÇ  ‚îú‚îÄ Spin up test environment (Docker)                          ‚îÇ
‚îÇ  ‚îú‚îÄ Test Glue jobs (mocked)                                    ‚îÇ
‚îÇ  ‚îú‚îÄ Test Lambda functions                                      ‚îÇ
‚îÇ  ‚îî‚îÄ Test Kafka producers/consumers                             ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Step 4: Data Quality Tests                                     ‚îÇ
‚îÇ  ‚îú‚îÄ Great Expectations validation                              ‚îÇ
‚îÇ  ‚îú‚îÄ Schema validation                                           ‚îÇ
‚îÇ  ‚îî‚îÄ Sample data pipeline run                                    ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Step 5: Build Artifacts                                        ‚îÇ
‚îÇ  ‚îú‚îÄ Package Glue scripts                                       ‚îÇ
‚îÇ  ‚îú‚îÄ Build Lambda deployment packages                           ‚îÇ
‚îÇ  ‚îú‚îÄ Build Docker images                                        ‚îÇ
‚îÇ  ‚îî‚îÄ Upload to S3/ECR                                           ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Result: ‚úÖ Pass ‚Üí Allow merge | ‚ùå Fail ‚Üí Block merge         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              CODE REVIEW & APPROVAL                             ‚îÇ
‚îÇ  ‚îú‚îÄ Peer review by Raj (Senior Engineer)                       ‚îÇ
‚îÇ  ‚îú‚îÄ Automated comments from CI results                         ‚îÇ
‚îÇ  ‚îú‚îÄ Approve & Merge to main branch                             ‚îÇ
‚îÇ  ‚îî‚îÄ Delete feature branch                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              GITHUB ACTIONS - CD PIPELINE                       ‚îÇ
‚îÇ              (Triggered on merge to main)                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  DEPLOY TO DEV ENVIRONMENT                                      ‚îÇ
‚îÇ  ‚îú‚îÄ Deploy Glue jobs (dev)                                     ‚îÇ
‚îÇ  ‚îú‚îÄ Deploy Lambda functions (dev)                              ‚îÇ
‚îÇ  ‚îú‚îÄ Run dbt models (dev Snowflake)                             ‚îÇ
‚îÇ  ‚îú‚îÄ Deploy SageMaker models (dev endpoint)                     ‚îÇ
‚îÇ  ‚îú‚îÄ Smoke tests                                                 ‚îÇ
‚îÇ  ‚îî‚îÄ ‚úÖ Dev deployment successful                               ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚Üì (Manual approval required)                                   ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  DEPLOY TO STAGING ENVIRONMENT                                  ‚îÇ
‚îÇ  ‚îú‚îÄ Terraform plan & apply (staging infra)                     ‚îÇ
‚îÇ  ‚îú‚îÄ Deploy all components to staging                           ‚îÇ
‚îÇ  ‚îú‚îÄ Run full integration test suite                            ‚îÇ
‚îÇ  ‚îú‚îÄ Run end-to-end data quality tests                          ‚îÇ
‚îÇ  ‚îú‚îÄ Performance testing                                         ‚îÇ
‚îÇ  ‚îî‚îÄ ‚úÖ Staging tests passed                                    ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚Üì (Manual approval required)                                   ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  DEPLOY TO PRODUCTION                                           ‚îÇ
‚îÇ  ‚îú‚îÄ Blue-Green deployment strategy                             ‚îÇ
‚îÇ  ‚îú‚îÄ Deploy to "green" environment                              ‚îÇ
‚îÇ  ‚îú‚îÄ Run production smoke tests                                 ‚îÇ
‚îÇ  ‚îú‚îÄ Monitor for 15 minutes                                     ‚îÇ
‚îÇ  ‚îú‚îÄ Switch traffic: blue ‚Üí green                               ‚îÇ
‚îÇ  ‚îú‚îÄ Keep blue as rollback option (1 hour)                      ‚îÇ
‚îÇ  ‚îî‚îÄ ‚úÖ Production deployment complete                          ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  NOTIFICATIONS                                                   ‚îÇ
‚îÇ  ‚îú‚îÄ Slack: #deployments channel                                ‚îÇ
‚îÇ  ‚îú‚îÄ Email: data-engineering@dpa.com                            ‚îÇ
‚îÇ  ‚îî‚îÄ JIRA: Auto-update ticket status                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ **STEP 1: GITHUB REPOSITORY STRUCTURE**

```
dpa-cpg-analytics/
‚îú‚îÄ .github/
‚îÇ  ‚îú‚îÄ workflows/
‚îÇ  ‚îÇ  ‚îú‚îÄ ci-pipeline.yml              # CI: Tests on PR
‚îÇ  ‚îÇ  ‚îú‚îÄ cd-dev.yml                   # CD: Deploy to dev
‚îÇ  ‚îÇ  ‚îú‚îÄ cd-staging.yml               # CD: Deploy to staging
‚îÇ  ‚îÇ  ‚îú‚îÄ cd-production.yml            # CD: Deploy to prod
‚îÇ  ‚îÇ  ‚îú‚îÄ infrastructure.yml           # Terraform deploy
‚îÇ  ‚îÇ  ‚îî‚îÄ data-quality-check.yml       # Data quality tests
‚îÇ  ‚îú‚îÄ pull_request_template.md        # PR template
‚îÇ  ‚îî‚îÄ CODEOWNERS                      # Auto-assign reviewers
‚îÇ
‚îú‚îÄ glue/
‚îÇ  ‚îú‚îÄ jobs/
‚îÇ  ‚îÇ  ‚îú‚îÄ bronze_promo_ingestion.py
‚îÇ  ‚îÇ  ‚îú‚îÄ silver_promo_transformation.py
‚îÇ  ‚îÇ  ‚îî‚îÄ gold_promo_aggregations.py
‚îÇ  ‚îî‚îÄ tests/
‚îÇ     ‚îú‚îÄ test_bronze_ingestion.py
‚îÇ     ‚îú‚îÄ test_silver_transformation.py
‚îÇ     ‚îî‚îÄ conftest.py                  # pytest fixtures
‚îÇ
‚îú‚îÄ lambda/
‚îÇ  ‚îú‚îÄ functions/
‚îÇ  ‚îÇ  ‚îú‚îÄ kafka_consumer/
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ lambda_function.py
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ requirements.txt
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ tests/
‚îÇ  ‚îÇ  ‚îÇ     ‚îî‚îÄ test_lambda_function.py
‚îÇ  ‚îÇ  ‚îî‚îÄ file_validator/
‚îÇ  ‚îÇ     ‚îú‚îÄ lambda_function.py
‚îÇ  ‚îÇ     ‚îî‚îÄ tests/
‚îÇ  ‚îî‚îÄ layers/                          # Shared dependencies
‚îÇ
‚îú‚îÄ dbt/
‚îÇ  ‚îú‚îÄ dpa_analytics/
‚îÇ  ‚îÇ  ‚îú‚îÄ models/
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ staging/
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ marts/
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ schema.yml
‚îÇ  ‚îÇ  ‚îú‚îÄ tests/
‚îÇ  ‚îÇ  ‚îî‚îÄ dbt_project.yml
‚îÇ  ‚îî‚îÄ profiles.yml
‚îÇ
‚îú‚îÄ sagemaker/
‚îÇ  ‚îú‚îÄ training/
‚îÇ  ‚îÇ  ‚îú‚îÄ train_promo_lift_model.py
‚îÇ  ‚îÇ  ‚îî‚îÄ requirements.txt
‚îÇ  ‚îú‚îÄ inference/
‚îÇ  ‚îÇ  ‚îî‚îÄ batch_inference.py
‚îÇ  ‚îî‚îÄ tests/
‚îÇ     ‚îî‚îÄ test_model_training.py
‚îÇ
‚îú‚îÄ airflow/
‚îÇ  ‚îú‚îÄ dags/
‚îÇ  ‚îÇ  ‚îú‚îÄ cpg_analytics_master_pipeline.py
‚îÇ  ‚îÇ  ‚îî‚îÄ weekly_ml_retraining.py
‚îÇ  ‚îî‚îÄ tests/
‚îÇ     ‚îî‚îÄ test_dags.py
‚îÇ
‚îú‚îÄ terraform/
‚îÇ  ‚îú‚îÄ environments/
‚îÇ  ‚îÇ  ‚îú‚îÄ dev/
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ main.tf
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ variables.tf
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ terraform.tfvars
‚îÇ  ‚îÇ  ‚îú‚îÄ staging/
‚îÇ  ‚îÇ  ‚îî‚îÄ production/
‚îÇ  ‚îú‚îÄ modules/
‚îÇ  ‚îÇ  ‚îú‚îÄ glue/
‚îÇ  ‚îÇ  ‚îú‚îÄ lambda/
‚îÇ  ‚îÇ  ‚îú‚îÄ msk/
‚îÇ  ‚îÇ  ‚îî‚îÄ sagemaker/
‚îÇ  ‚îî‚îÄ backend.tf                       # S3 backend for state
‚îÇ
‚îú‚îÄ tests/
‚îÇ  ‚îú‚îÄ integration/
‚îÇ  ‚îÇ  ‚îú‚îÄ test_end_to_end_pipeline.py
‚îÇ  ‚îÇ  ‚îî‚îÄ test_kafka_flow.py
‚îÇ  ‚îú‚îÄ data_quality/
‚îÇ  ‚îÇ  ‚îú‚îÄ great_expectations/
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ expectations/
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ checkpoints/
‚îÇ  ‚îÇ  ‚îî‚îÄ test_data_quality.py
‚îÇ  ‚îî‚îÄ fixtures/
‚îÇ     ‚îî‚îÄ sample_data.csv
‚îÇ
‚îú‚îÄ scripts/
‚îÇ  ‚îú‚îÄ deploy_glue_jobs.sh
‚îÇ  ‚îú‚îÄ deploy_lambda.sh
‚îÇ  ‚îú‚îÄ run_dbt.sh
‚îÇ  ‚îî‚îÄ smoke_tests.sh
‚îÇ
‚îú‚îÄ docs/
‚îÇ  ‚îú‚îÄ architecture.md
‚îÇ  ‚îú‚îÄ deployment.md
‚îÇ  ‚îî‚îÄ runbooks/
‚îÇ
‚îú‚îÄ .gitignore
‚îú‚îÄ .pylintrc
‚îú‚îÄ .flake8
‚îú‚îÄ pyproject.toml                      # Black, isort config
‚îú‚îÄ pytest.ini
‚îú‚îÄ requirements.txt                    # Dev dependencies
‚îú‚îÄ requirements-test.txt               # Test dependencies
‚îú‚îÄ README.md
‚îî‚îÄ CHANGELOG.md
```

---

## üîß **STEP 2: CREATE GITHUB ACTIONS WORKFLOWS**

### **Workflow 1: CI Pipeline (Pull Request)**

**File: `.github/workflows/ci-pipeline.yml`**

```yaml
name: CI Pipeline - Tests & Quality Checks

on:
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'glue/**'
      - 'lambda/**'
      - 'dbt/**'
      - 'sagemaker/**'
      - 'airflow/**'
      - 'tests/**'
      - '.github/workflows/**'

env:
  PYTHON_VERSION: '3.11'
  AWS_REGION: us-east-1

jobs:
  # ==========================================================================
  # JOB 1: CODE QUALITY CHECKS
  # ==========================================================================
  code-quality:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: üì¶ Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements-test.txt
          pip install pylint flake8 black isort bandit safety sqlfluff
      
      - name: üé® Check code formatting (Black)
        run: |
          black --check --diff glue/ lambda/ sagemaker/ airflow/
      
      - name: üìê Check import sorting (isort)
        run: |
          isort --check-only --diff glue/ lambda/ sagemaker/ airflow/
      
      - name: üîç Lint Python code (Pylint)
        run: |
          pylint glue/ lambda/ sagemaker/ airflow/ --fail-under=8.0
      
      - name: üìè Lint Python code (Flake8)
        run: |
          flake8 glue/ lambda/ sagemaker/ airflow/ --max-line-length=120
      
      - name: üîí Security scan (Bandit)
        run: |
          bandit -r glue/ lambda/ sagemaker/ airflow/ -ll
      
      - name: üõ°Ô∏è Dependency security scan (Safety)
        run: |
          safety check --json
        continue-on-error: true
      
      - name: üìä SQL Lint (sqlfluff)
        run: |
          sqlfluff lint dbt/dpa_analytics/models/ --dialect snowflake

  # ==========================================================================
  # JOB 2: UNIT TESTS
  # ==========================================================================
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: code-quality
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: üì¶ Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-test.txt
          pip install pytest pytest-cov pytest-mock
      
      - name: üß™ Run Glue job tests
        run: |
          pytest glue/tests/ -v --cov=glue/jobs --cov-report=xml --cov-report=term
      
      - name: üß™ Run Lambda tests
        run: |
          pytest lambda/ -v --cov=lambda/functions --cov-report=xml --cov-append
      
      - name: üß™ Run SageMaker tests
        run: |
          pytest sagemaker/tests/ -v --cov=sagemaker --cov-report=xml --cov-append
      
      - name: üìä Upload coverage to CodeCov
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: true
      
      - name: ‚úÖ Check coverage threshold
        run: |
          coverage report --fail-under=80

  # ==========================================================================
  # JOB 3: INTEGRATION TESTS
  # ==========================================================================
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    services:
      # Mock Kafka for testing
      kafka:
        image: confluentinc/cp-kafka:7.5.0
        ports:
          - 9092:9092
        env:
          KAFKA_BROKER_ID: 1
          KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      
      # Mock PostgreSQL for Airflow metadata
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
        ports:
          - 5432:5432
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: üì¶ Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: üß™ Run integration tests
        env:
          KAFKA_BOOTSTRAP_SERVERS: localhost:9092
          POSTGRES_HOST: localhost
        run: |
          pytest tests/integration/ -v --maxfail=1
      
      - name: üß™ Test Airflow DAGs
        run: |
          pytest airflow/tests/ -v

  # ==========================================================================
  # JOB 4: DATA QUALITY TESTS
  # ==========================================================================
  data-quality:
    name: Data Quality Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: üì¶ Install Great Expectations
        run: |
          pip install great-expectations==0.18.0
      
      - name: ‚úÖ Run Great Expectations
        run: |
          great_expectations checkpoint run bronze_checkpoint
          great_expectations checkpoint run silver_checkpoint
        working-directory: tests/data_quality
      
      - name: üìä Generate data quality report
        run: |
          great_expectations docs build
        working-directory: tests/data_quality

  # ==========================================================================
  # JOB 5: DBT TESTS
  # ==========================================================================
  dbt-tests:
    name: dbt Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    env:
      DBT_PROFILES_DIR: ./dbt
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: üì¶ Install dbt
        run: |
          pip install dbt-snowflake==1.7.0
      
      - name: üîß Configure Snowflake credentials
        run: |
          mkdir -p ~/.dbt
          cat > ~/.dbt/profiles.yml <<EOF
          dpa_analytics:
            target: ci
            outputs:
              ci:
                type: snowflake
                account: ${{ secrets.SNOWFLAKE_ACCOUNT }}
                user: ${{ secrets.SNOWFLAKE_CI_USER }}
                password: ${{ secrets.SNOWFLAKE_CI_PASSWORD }}
                role: CI_ROLE
                database: ANALYTICS_CI
                warehouse: COMPUTE_WH_CI
                schema: DBT_CI
                threads: 4
          EOF
      
      - name: üì¶ Install dbt dependencies
        run: |
          dbt deps
        working-directory: dbt/dpa_analytics
      
      - name: üîç dbt compile
        run: |
          dbt compile --select state:modified+
        working-directory: dbt/dpa_analytics
      
      - name: üß™ dbt test
        run: |
          dbt test --select state:modified+
        working-directory: dbt/dpa_analytics

  # ==========================================================================
  # JOB 6: BUILD ARTIFACTS
  # ==========================================================================
  build:
    name: Build Deployment Artifacts
    runs-on: ubuntu-latest
    needs: [integration-tests, data-quality, dbt-tests]
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: üì¶ Package Glue jobs
        run: |
          mkdir -p artifacts/glue
          cp -r glue/jobs/* artifacts/glue/
          zip -r artifacts/glue-jobs.zip artifacts/glue/
      
      - name: üì¶ Package Lambda functions
        run: |
          for func in lambda/functions/*/; do
            func_name=$(basename $func)
            mkdir -p artifacts/lambda/$func_name
            cd $func
            pip install -r requirements.txt -t .
            zip -r ../../../artifacts/lambda/$func_name.zip .
            cd -
          done
      
      - name: üì¶ Package dbt project
        run: |
          cd dbt/dpa_analytics
          dbt deps
          cd -
          zip -r artifacts/dbt-project.zip dbt/
      
      - name: üì§ Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: deployment-artifacts
          path: artifacts/
          retention-days: 30

  # ==========================================================================
  # JOB 7: SUMMARY & NOTIFICATIONS
  # ==========================================================================
  pr-summary:
    name: PR Summary
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests, integration-tests, data-quality, dbt-tests, build]
    if: always()
    
    steps:
      - name: üìä Create PR comment with results
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const results = {
              'Code Quality': '${{ needs.code-quality.result }}',
              'Unit Tests': '${{ needs.unit-tests.result }}',
              'Integration Tests': '${{ needs.integration-tests.result }}',
              'Data Quality': '${{ needs.data-quality.result }}',
              'dbt Tests': '${{ needs.dbt-tests.result }}',
              'Build': '${{ needs.build.result }}'
            };
            
            let comment = '## ü§ñ CI Pipeline Results\n\n';
            
            for (const [job, result] of Object.entries(results)) {
              const emoji = result === 'success' ? '‚úÖ' : result === 'failure' ? '‚ùå' : '‚è≠Ô∏è';
              comment += `${emoji} **${job}**: ${result}\n`;
            }
            
            comment += '\n---\n';
            comment += `**Workflow Run**: [View Details](${context.payload.pull_request.html_url}/checks)\n`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
      
      - name: üì¢ Notify Slack
        uses: slackapi/slack-github-action@v1
        if: failure()
        with:
          channel-id: 'C01234567'  # #ci-failures channel
          slack-message: |
            ‚ùå CI Pipeline Failed
            PR: ${{ github.event.pull_request.title }}
            Author: ${{ github.event.pull_request.user.login }}
            URL: ${{ github.event.pull_request.html_url }}
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
```

---

### **Workflow 2: CD - Deploy to Dev**

**File: `.github/workflows/cd-dev.yml`**

```yaml
name: CD - Deploy to Dev

on:
  push:
    branches: [ main ]
  workflow_dispatch:  # Manual trigger

env:
  AWS_REGION: us-east-1
  ENVIRONMENT: dev

jobs:
  deploy-dev:
    name: Deploy to Dev Environment
    runs-on: ubuntu-latest
    environment: 
      name: dev
      url: https://dev.dpa-analytics.com
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üîê Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      # ====================================================================
      # DEPLOY GLUE JOBS
      # ====================================================================
      - name: üöÄ Deploy Glue Jobs
        run: |
          # Upload scripts to S3
          aws s3 sync glue/jobs/ s3://dpa-glue-scripts-dev/jobs/ \
            --delete \
            --exclude "*.pyc" \
            --exclude "__pycache__/*"
          
          # Update Glue jobs
          for job in bronze-promo-ingestion silver-promo-transformation gold-promo-aggregations; do
            echo "Updating Glue job: $job-dev"
            aws glue update-job \
              --job-name "$job-dev" \
              --job-update '{
                "Command": {
                  "Name": "glueetl",
                  "ScriptLocation": "s3://dpa-glue-scripts-dev/jobs/'$job'.py"
                }
              }'
          done
      
      # ====================================================================
      # DEPLOY LAMBDA FUNCTIONS
      # ====================================================================
      - name: üöÄ Deploy Lambda Functions
        run: |
          for func_dir in lambda/functions/*/; do
            func_name=$(basename $func_dir)
            
            echo "Deploying Lambda: $func_name-dev"
            
            # Create deployment package
            cd $func_dir
            rm -rf package
            mkdir package
            pip install -r requirements.txt -t package/
            cp lambda_function.py package/
            cd package
            zip -r ../function.zip .
            cd ..
            
            # Update Lambda function
            aws lambda update-function-code \
              --function-name "$func_name-dev" \
              --zip-file fileb://function.zip
            
            cd -
          done
      
      # ====================================================================
      # RUN DBT MODELS
      # ====================================================================
      - name: üöÄ Run dbt Models (Dev)
        env:
          DBT_PROFILES_DIR: ./dbt
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_DEV_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_DEV_PASSWORD }}
        run: |
          pip install dbt-snowflake
          
          cd dbt/dpa_analytics
          dbt deps
          dbt run --target dev
          dbt test --target dev
      
      # ====================================================================
      # DEPLOY AIRFLOW DAGS
      # ====================================================================
      - name: üöÄ Deploy Airflow DAGs
        run: |
          aws s3 sync airflow/dags/ s3://dpa-airflow-dags-dev/dags/ \
            --delete
      
      # ====================================================================
      # SMOKE TESTS
      # ====================================================================
      - name: üß™ Run Smoke Tests
        run: |
          python scripts/smoke_tests.py --env dev
      
      # ====================================================================
      # NOTIFICATIONS
      # ====================================================================
      - name: üì¢ Notify Slack - Success
        if: success()
        uses: slackapi/slack-github-action@v1
        with:
          channel-id: 'C01234567'  # #deployments
          slack-message: |
            ‚úÖ **Dev Deployment Successful**
            Environment: Dev
            Commit: ${{ github.sha }}
            Author: ${{ github.actor }}
            Time: ${{ github.event.head_commit.timestamp }}
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
      
      - name: üì¢ Notify Slack - Failure
        if: failure()
        uses: slackapi/slack-github-action@v1
        with:
          channel-id: 'C01234567'
          slack-message: |
            ‚ùå **Dev Deployment Failed**
            Environment: Dev
            Commit: ${{ github.sha }}
            Author: ${{ github.actor }}
            Logs: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
```

---

### **Workflow 3: CD - Deploy to Production (with approvals)**

**File: `.github/workflows/cd-production.yml`**

```yaml
name: CD - Deploy to Production

on:
  workflow_dispatch:  # Manual trigger only
    inputs:
      version:
        description: 'Version to deploy (e.g., v1.2.3)'
        required: true
        type: string

env:
  AWS_REGION: us-east-1
  ENVIRONMENT: production

jobs:
  # ==========================================================================
  # STAGE 1: PRE-DEPLOYMENT CHECKS
  # ==========================================================================
  pre-deployment-checks:
    name: Pre-Deployment Validation
    runs-on: ubuntu-latest
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.version }}
      
      - name: ‚úÖ Verify version tag exists
        run: |
          if ! git rev-parse ${{ github.event.inputs.version }} >/dev/null 2>&1; then
            echo "‚ùå Version tag ${{ github.event.inputs.version }} does not exist"
            exit 1
          fi
          echo "‚úÖ Version tag verified"
      
      - name: ‚úÖ Check all tests passed
        run: |
          # Verify CI passed on this commit
          echo "Checking CI status..."
          # Add logic to verify CI status via GitHub API
      
      - name: ‚úÖ Check staging deployment
        run: |
          echo "Verifying staging deployment was successful..."
          # Add logic to check staging

  # ==========================================================================
  # STAGE 2: MANUAL APPROVAL (Required)
  # ==========================================================================
  approval:
    name: Production Deployment Approval
    runs-on: ubuntu-latest
    needs: pre-deployment-checks
    environment:
      name: production-approval
    
    steps:
      - name: ‚è∏Ô∏è  Waiting for approval
        run: |
          echo "Deployment to production requires manual approval"
          echo "Approvers: Raj, Sarah, Director of Engineering"

  # ==========================================================================
  # STAGE 3: BLUE-GREEN DEPLOYMENT
  # ==========================================================================
  deploy-green:
    name: Deploy to Green Environment
    runs-on: ubuntu-latest
    needs: approval
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.version }}
      
      - name: üîê Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: üöÄ Deploy to Green environment
        run: |
          echo "Deploying version ${{ github.event.inputs.version }} to GREEN"
          
          # Deploy all components to green environment
          # (Similar to dev deployment but with -prod-green suffix)
          
          # Deploy Glue jobs
          aws s3 sync glue/jobs/ s3://dpa-glue-scripts-prod-green/jobs/
          
          # Deploy Lambda
          for func_dir in lambda/functions/*/; do
            func_name=$(basename $func_dir)
            # Deploy to green version
            echo "Deploying $func_name to green"
          done
      
      - name: üß™ Run production smoke tests (Green)
        run: |
          python scripts/smoke_tests.py --env production-green
      
      - name: üìä Monitor green environment
        run: |
          echo "Monitoring green environment for 15 minutes..."
          python scripts/monitor_deployment.py --env green --duration 900

  # ==========================================================================
  # STAGE 4: TRAFFIC SWITCH
  # ==========================================================================
  switch-traffic:
    name: Switch Traffic to Green
    runs-on: ubuntu-latest
    needs: deploy-green
    
    steps:
      - name: üîÄ Update Route53 / ALB to point to green
        run: |
          echo "Switching traffic from blue to green..."
          
          # Update ALB target group or Route53 weighted routing
          aws elbv2 modify-listener \
            --listener-arn arn:aws:elasticloadbalancing:... \
            --default-actions Type=forward,TargetGroupArn=arn:aws:elasticloadbalancing:.../green
          
          echo "‚úÖ Traffic switched to green"
      
      - name: ‚è≥ Monitor for issues (15 min)
        run: |
          python scripts/monitor_production.py --duration 900
      
      - name: ‚úÖ Mark green as new blue
        run: |
          echo "Green is now the active production environment"
          echo "Old blue kept as rollback option for 1 hour"

  # ==========================================================================
  # STAGE 5: POST-DEPLOYMENT
  # ==========================================================================
  post-deployment:
    name: Post-Deployment Tasks
    runs-on: ubuntu-latest
    needs: switch-traffic
    
    steps:
      - name: üìä Update deployment metrics
        run: |
          # Log deployment to metrics system
          curl -X POST https://metrics.dpa.com/api/deployments \
            -H "Content-Type: application/json" \
            -d '{
              "environment": "production",
              "version": "${{ github.event.inputs.version }}",
              "deployed_by": "${{ github.actor }}",
              "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"
            }'
      
      - name: üìù Update JIRA tickets
        run: |
          # Auto-close deployed tickets
          python scripts/update_jira.py --version ${{ github.event.inputs.version }}
      
      - name: üì¢ Send deployment announcement
        uses: slackapi/slack-github-action@v1
        with:
          channel-id: 'C01234567'  # #general
          slack-message: |
            üéâ **Production Deployment Complete!**
            
            Version: ${{ github.event.inputs.version }}
            Deployed by: ${{ github.actor }}
            Time: $(date -u +%Y-%m-%d\ %H:%M:%S\ UTC)
            
            What's New:
            - Improved promotional analytics
            - Enhanced ML model accuracy
            - Bug fixes and performance improvements
            
            [View Release Notes](https://github.com/${{ github.repository }}/releases/tag/${{ github.event.inputs.version }})
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}

  # ==========================================================================
  # ROLLBACK JOB (Manual trigger if needed)
  # ==========================================================================
  rollback:
    name: Rollback to Blue
    runs-on: ubuntu-latest
    if: failure()
    needs: switch-traffic
    
    steps:
      - name: üîô Rollback to blue environment
        run: |
          echo "‚ùå Issues detected - Rolling back to blue"
          
          aws elbv2 modify-listener \
            --listener-arn arn:aws:elasticloadbalancing:... \
            --default-actions Type=forward,TargetGroupArn=arn:aws:elasticloadbalancing:.../blue
          
          echo "‚úÖ Rolled back to previous version"
      
      - name: üö® Send rollback alert
        uses: slackapi/slack-github-action@v1
        with:
          channel-id: 'C01234567'
          slack-message: |
            üö® **PRODUCTION ROLLBACK EXECUTED**
            
            Version attempted: ${{ github.event.inputs.version }}
            Rolled back to: Previous stable version
            Triggered by: Automated failure detection
            
            @channel - Production team please investigate immediately
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
```

---

Due to length limits, should I continue with:

1. **Infrastructure as Code (Terraform workflows)**
2. **Complete testing setup (pytest, Great Expectations)**
3. **GitHub Actions secrets management**
4. **Deployment scripts**
5. **Monitoring & rollback automation**

Which part would you like next? üöÄ
