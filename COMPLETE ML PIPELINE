# ğŸ¤– COMPLETE ML PIPELINE - SAGEMAKER (PROMOTIONAL LIFT PREDICTION)

Let me show you the **COMPLETE END-TO-END ML PIPELINE** using Amazon SageMaker to predict promotional sales lift for CPG!

---

## ğŸ“‹ **JIRA EPIC: ML-POWERED PROMOTIONAL ANALYTICS**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  EPIC-50: ML-Powered Promotional Sales Lift Prediction       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                               â•‘
â•‘  Business Objective:                                          â•‘
â•‘  Predict sales lift % for promotions to optimize marketing   â•‘
â•‘  spend and maximize ROI for Unilever campaigns               â•‘
â•‘                                                               â•‘
â•‘  Stories:                                                     â•‘
â•‘  â”œâ”€ CPG-501: Feature Engineering (Snowflake Snowpark) [13]  â•‘
â•‘  â”œâ”€ CPG-502: Model Training (SageMaker)           [13]       â•‘
â•‘  â”œâ”€ CPG-503: Model Registry & Versioning          [5]        â•‘
â•‘  â”œâ”€ CPG-504: Batch Inference Pipeline             [8]        â•‘
â•‘  â”œâ”€ CPG-505: Real-time Endpoint Deployment        [8]        â•‘
â•‘  â”œâ”€ CPG-506: Model Monitoring & Drift Detection   [8]        â•‘
â•‘  â””â”€ CPG-507: MLOps Automation (Pipelines)         [13]       â•‘
â•‘                                                               â•‘
â•‘  Total: 68 story points (3-4 sprints)                        â•‘
â•‘  Team: You + Maria (ML focus)                                â•‘
â•‘                                                               â•‘
â•‘  Expected Business Impact:                                    â•‘
â•‘  â€¢ 15-20% improvement in promo ROI prediction                â•‘
â•‘  â€¢ $5M+ annual savings in marketing spend                    â•‘
â•‘  â€¢ Faster decision-making (real-time predictions)            â•‘
â•‘                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ **ML USE CASE: PROMOTIONAL SALES LIFT PREDICTION**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  PROBLEM STATEMENT                                            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                               â•‘
â•‘  Current State:                                               â•‘
â•‘  Unilever runs promotions without knowing expected lift      â•‘
â•‘  â†’ Overspend on low-performing campaigns                     â•‘
â•‘  â†’ Miss opportunities on high-potential campaigns            â•‘
â•‘  â†’ No data-driven optimization                               â•‘
â•‘                                                               â•‘
â•‘  Desired State:                                               â•‘
â•‘  Predict sales lift % BEFORE running promotion               â•‘
â•‘  â†’ Optimize discount levels                                  â•‘
â•‘  â†’ Allocate budget efficiently                               â•‘
â•‘  â†’ Maximize ROI                                              â•‘
â•‘                                                               â•‘
â•‘  ML Solution:                                                 â•‘
â•‘  Regression model to predict:                                â•‘
â•‘  Sales Lift % = (Promo Sales - Baseline) / Baseline * 100   â•‘
â•‘                                                               â•‘
â•‘  Input Features:                                              â•‘
â•‘  â€¢ Product attributes (brand, category, price)               â•‘
â•‘  â€¢ Promotion details (discount %, type, duration)            â•‘
â•‘  â€¢ Store characteristics (region, size, foot traffic)        â•‘
â•‘  â€¢ Historical sales trends (7-day avg, seasonality)          â•‘
â•‘  â€¢ Temporal features (day of week, holiday, season)          â•‘
â•‘  â€¢ Competitor activity indicators                            â•‘
â•‘                                                               â•‘
â•‘  Target Variable:                                             â•‘
â•‘  sales_lift_pct (continuous, -100% to +500%)                 â•‘
â•‘                                                               â•‘
â•‘  Success Metrics:                                             â•‘
â•‘  â€¢ RMSE < 8% lift prediction error                           â•‘
â•‘  â€¢ RÂ² > 0.75 (75% variance explained)                        â•‘
â•‘  â€¢ MAE < 5% average error                                    â•‘
â•‘                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ—ï¸ **COMPLETE ML ARCHITECTURE**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 SNOWFLAKE (Data Source)                         â”‚
â”‚  Gold Layer: Product performance, historical sales              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FEATURE ENGINEERING (Snowflake Snowpark)                â”‚
â”‚  â€¢ Join historical sales with promotions                        â”‚
â”‚  â€¢ Calculate baseline sales (7-day avg)                         â”‚
â”‚  â€¢ Calculate actual sales lift %                                â”‚
â”‚  â€¢ Create ML features (100+ features)                           â”‚
â”‚  â€¢ Train/test split (80/20, time-based)                         â”‚
â”‚  â€¢ Export to S3                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SAGEMAKER FEATURE STORE (Optional)                      â”‚
â”‚  Centralized feature repository for consistency                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SAGEMAKER TRAINING (AutoML / Custom Algorithm)          â”‚
â”‚  â€¢ Algorithm: XGBoost (AWS built-in)                            â”‚
â”‚  â€¢ Hyperparameter tuning with SageMaker Tuner                   â”‚
â”‚  â€¢ Cross-validation                                              â”‚
â”‚  â€¢ Model evaluation metrics                                     â”‚
â”‚  â€¢ Model artifacts stored in S3                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SAGEMAKER MODEL REGISTRY                                â”‚
â”‚  â€¢ Model versioning (v1, v2, v3...)                             â”‚
â”‚  â€¢ Model approval workflow                                       â”‚
â”‚  â€¢ Model lineage tracking                                        â”‚
â”‚  â€¢ A/B testing support                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â–¼                 â–¼                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BATCH INFERENCE  â”‚  â”‚ REAL-TIME ENDPOINT  â”‚  â”‚ BATCH TRANSFORM â”‚
â”‚ (Scheduled)      â”‚  â”‚ (Low latency API)   â”‚  â”‚ (Ad-hoc scoring)â”‚
â”‚                  â”‚  â”‚                     â”‚  â”‚                 â”‚
â”‚ â€¢ Daily batch    â”‚  â”‚ â€¢ REST API          â”‚  â”‚ â€¢ Large batches â”‚
â”‚ â€¢ Predict all    â”‚  â”‚ â€¢ <100ms latency    â”‚  â”‚ â€¢ On-demand     â”‚
â”‚   upcoming       â”‚  â”‚ â€¢ Auto-scaling      â”‚  â”‚                 â”‚
â”‚   promotions     â”‚  â”‚                     â”‚  â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                 â”‚                                  â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PREDICTIONS STORAGE (Snowflake)                         â”‚
â”‚  Table: ML_PREDICTIONS.PROMO_LIFT_PREDICTIONS                   â”‚
â”‚  â€¢ promotion_id                                                  â”‚
â”‚  â€¢ predicted_lift_pct                                            â”‚
â”‚  â€¢ prediction_confidence                                         â”‚
â”‚  â€¢ model_version                                                 â”‚
â”‚  â€¢ prediction_timestamp                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         MODEL MONITORING (SageMaker Model Monitor)              â”‚
â”‚  â€¢ Data quality monitoring                                       â”‚
â”‚  â€¢ Model quality monitoring (accuracy drift)                     â”‚
â”‚  â€¢ Feature drift detection                                       â”‚
â”‚  â€¢ Alerts on degradation                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“… **CPG-501: FEATURE ENGINEERING WITH SNOWFLAKE SNOWPARK**

### **Create Training Dataset in Snowflake:**

```sql
-- ============================================================================
-- FEATURE ENGINEERING FOR ML: PROMOTIONAL SALES LIFT
-- ============================================================================
-- Purpose: Create ML training dataset with features and target variable
-- Author: You + Maria
-- Date: 2024-02-11
-- ============================================================================

USE ROLE DATA_ENGINEER;
USE WAREHOUSE COMPUTE_WH_ETL;
USE DATABASE ANALYTICS_PROD;
USE SCHEMA ML_FEATURES;

-- Create schema for ML features
CREATE SCHEMA IF NOT EXISTS ML_FEATURES
    COMMENT = 'Feature engineering for machine learning models';

-- ============================================================================
-- STEP 1: CREATE HISTORICAL SALES TABLE (Simulated for this example)
-- ============================================================================

-- In production, this would come from your actual sales transactions
CREATE OR REPLACE TABLE HISTORICAL_SALES AS
SELECT
    promotion_id,
    product_id,
    store_id,
    transaction_date,
    daily_sales,
    daily_quantity,
    has_promotion,
    -- Baseline sales = 7-day avg before promotion (non-promo days)
    AVG(CASE WHEN has_promotion = 0 THEN daily_sales END) 
        OVER (
            PARTITION BY product_id, store_id 
            ORDER BY transaction_date 
            ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING
        ) AS baseline_sales_7day_avg,
    -- 7-day rolling metrics
    AVG(daily_sales) OVER (
        PARTITION BY product_id, store_id 
        ORDER BY transaction_date 
        ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING
    ) AS sales_7day_avg,
    STDDEV(daily_sales) OVER (
        PARTITION BY product_id, store_id 
        ORDER BY transaction_date 
        ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING
    ) AS sales_7day_std
FROM (
    -- Simulated daily sales data
    -- In production, this comes from your actual transactional data
    SELECT 
        'PROMO_' || LPAD(SEQ4(), 5, '0') AS promotion_id,
        'SKU_' || UNIFORM(1000, 9999, RANDOM()) AS product_id,
        'ST_' || LPAD(UNIFORM(1, 100, RANDOM()), 4, '0') AS store_id,
        DATEADD(day, -SEQ4(), CURRENT_DATE()) AS transaction_date,
        UNIFORM(100, 5000, RANDOM()) AS daily_sales,
        UNIFORM(10, 500, RANDOM()) AS daily_quantity,
        CASE WHEN UNIFORM(0, 100, RANDOM()) < 30 THEN 1 ELSE 0 END AS has_promotion
    FROM TABLE(GENERATOR(ROWCOUNT => 100000))
);

-- ============================================================================
-- STEP 2: JOIN PROMOTIONS WITH HISTORICAL SALES TO CALCULATE LIFT
-- ============================================================================

CREATE OR REPLACE TABLE ML_TRAINING_DATASET AS
WITH promotion_sales AS (
    -- Get sales during promotion period
    SELECT 
        p.promotion_id,
        p.product_id,
        p.store_id,
        p.start_date AS promo_start_date,
        p.end_date AS promo_end_date,
        
        -- Promotion attributes
        p.promo_type,
        p.discount_percentage,
        p.discount_category,
        p.budget_usd,
        p.budget_per_day,
        p.promo_duration_days,
        p.region,
        p.state,
        p.channel,
        p.season,
        p.promo_start_month,
        p.promo_start_day_of_week,
        p.is_weekend_start,
        p.is_holiday_aligned,
        p.promo_intensity_score,
        
        -- Product attributes
        p.product_name,
        p.brand,
        p.category,
        p.subcategory,
        p.unit_price AS master_unit_price,
        p.cost_price,
        p.discounted_price,
        p.margin_before_promo,
        p.margin_after_promo,
        
        -- Store attributes
        p.store_name,
        p.store_type,
        p.size_sqft AS store_size_sqft,
        p.avg_daily_foottraffic,
        p.potential_customer_reach,
        
        -- Sales metrics during promotion
        AVG(s.daily_sales) AS avg_daily_sales_during_promo,
        SUM(s.daily_sales) AS total_sales_during_promo,
        AVG(s.daily_quantity) AS avg_daily_quantity_during_promo,
        
        -- Baseline sales (from before promotion)
        AVG(s.baseline_sales_7day_avg) AS baseline_sales,
        AVG(s.sales_7day_std) AS sales_volatility
        
    FROM ANALYTICS_PROD.GOLD.PROMO_ROI_DASHBOARD p
    INNER JOIN HISTORICAL_SALES s
        ON p.promotion_id = s.promotion_id
        AND s.transaction_date BETWEEN p.start_date AND p.end_date
    WHERE p.promo_status = 'Completed'  -- Only completed promotions
    GROUP BY 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32
),

feature_engineering AS (
    SELECT
        promotion_id,
        
        -- ================================================================
        -- TARGET VARIABLE: Sales Lift %
        -- ================================================================
        ROUND(
            CASE 
                WHEN baseline_sales > 0 THEN
                    ((avg_daily_sales_during_promo - baseline_sales) / baseline_sales) * 100
                ELSE NULL
            END,
        2) AS sales_lift_pct,
        
        -- ================================================================
        -- FEATURES: Promotion Characteristics
        -- ================================================================
        promo_type,
        discount_percentage,
        discount_category,
        budget_usd,
        budget_per_day,
        promo_duration_days,
        promo_intensity_score,
        
        -- Timing features
        promo_start_month,
        promo_start_day_of_week,
        is_weekend_start,
        is_holiday_aligned,
        season,
        
        -- ================================================================
        -- FEATURES: Product Characteristics
        -- ================================================================
        brand,
        category,
        subcategory,
        master_unit_price,
        cost_price,
        discounted_price,
        margin_before_promo,
        margin_after_promo,
        
        -- Price-related features
        ROUND((master_unit_price - discounted_price) / master_unit_price * 100, 2) AS effective_discount_pct,
        ROUND(master_unit_price / cost_price, 2) AS price_to_cost_ratio,
        
        -- ================================================================
        -- FEATURES: Store Characteristics
        -- ================================================================
        region,
        state,
        channel,
        store_type,
        store_size_sqft,
        avg_daily_foottraffic,
        potential_customer_reach,
        
        -- ================================================================
        -- FEATURES: Historical Sales Patterns
        -- ================================================================
        baseline_sales,
        sales_volatility,
        ROUND(sales_volatility / NULLIF(baseline_sales, 0), 4) AS coefficient_of_variation,
        
        -- ================================================================
        -- FEATURES: Interaction Terms
        -- ================================================================
        discount_percentage * promo_duration_days AS discount_x_duration,
        discount_percentage * is_weekend_start AS discount_x_weekend,
        master_unit_price * discount_percentage AS price_x_discount,
        
        -- ================================================================
        -- METADATA
        -- ================================================================
        CURRENT_TIMESTAMP() AS feature_created_at
        
    FROM promotion_sales
    WHERE baseline_sales IS NOT NULL
        AND baseline_sales > 0
        AND avg_daily_sales_during_promo IS NOT NULL
)

SELECT * FROM feature_engineering
WHERE sales_lift_pct IS NOT NULL
    AND sales_lift_pct BETWEEN -50 AND 500;  -- Filter extreme outliers

-- ============================================================================
-- STEP 3: DATA QUALITY CHECKS
-- ============================================================================

-- Check data quality
SELECT
    COUNT(*) AS total_records,
    COUNT(DISTINCT promotion_id) AS unique_promotions,
    
    -- Target variable stats
    ROUND(AVG(sales_lift_pct), 2) AS avg_lift,
    ROUND(STDDEV(sales_lift_pct), 2) AS std_lift,
    ROUND(MIN(sales_lift_pct), 2) AS min_lift,
    ROUND(MAX(sales_lift_pct), 2) AS max_lift,
    ROUND(MEDIAN(sales_lift_pct), 2) AS median_lift,
    
    -- Check for nulls
    SUM(CASE WHEN sales_lift_pct IS NULL THEN 1 ELSE 0 END) AS null_target_count,
    SUM(CASE WHEN discount_percentage IS NULL THEN 1 ELSE 0 END) AS null_discount_count,
    
    -- Distribution
    SUM(CASE WHEN sales_lift_pct < 0 THEN 1 ELSE 0 END) AS negative_lift_count,
    SUM(CASE WHEN sales_lift_pct BETWEEN 0 AND 25 THEN 1 ELSE 0 END) AS low_lift_count,
    SUM(CASE WHEN sales_lift_pct BETWEEN 25 AND 50 THEN 1 ELSE 0 END) AS medium_lift_count,
    SUM(CASE WHEN sales_lift_pct > 50 THEN 1 ELSE 0 END) AS high_lift_count
    
FROM ML_TRAINING_DATASET;

-- ============================================================================
-- STEP 4: CREATE TRAIN/TEST SPLIT (80/20, time-based)
-- ============================================================================

-- Add random split column
CREATE OR REPLACE TABLE ML_TRAINING_DATASET_SPLIT AS
SELECT 
    *,
    CASE 
        WHEN UNIFORM(0, 100, RANDOM()) < 80 THEN 'TRAIN'
        ELSE 'TEST'
    END AS dataset_split
FROM ML_TRAINING_DATASET;

-- Verify split
SELECT 
    dataset_split,
    COUNT(*) AS record_count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) AS percentage
FROM ML_TRAINING_DATASET_SPLIT
GROUP BY dataset_split;

-- ============================================================================
-- STEP 5: EXPORT TO S3 FOR SAGEMAKER
-- ============================================================================

-- Export train set
COPY INTO @ML_FEATURES.S3_ML_STAGE/train/
FROM (
    SELECT
        sales_lift_pct,  -- Target first (XGBoost expects this)
        discount_percentage,
        promo_duration_days,
        budget_per_day,
        promo_intensity_score,
        promo_start_month,
        promo_start_day_of_week,
        is_weekend_start,
        is_holiday_aligned,
        master_unit_price,
        cost_price,
        effective_discount_pct,
        price_to_cost_ratio,
        store_size_sqft,
        avg_daily_foottraffic,
        baseline_sales,
        sales_volatility,
        coefficient_of_variation,
        discount_x_duration,
        discount_x_weekend,
        price_x_discount
    FROM ML_TRAINING_DATASET_SPLIT
    WHERE dataset_split = 'TRAIN'
)
FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY = '"' COMPRESSION = GZIP)
HEADER = FALSE
OVERWRITE = TRUE;

-- Export test set
COPY INTO @ML_FEATURES.S3_ML_STAGE/test/
FROM (
    SELECT
        sales_lift_pct,
        discount_percentage,
        promo_duration_days,
        budget_per_day,
        promo_intensity_score,
        promo_start_month,
        promo_start_day_of_week,
        is_weekend_start,
        is_holiday_aligned,
        master_unit_price,
        cost_price,
        effective_discount_pct,
        price_to_cost_ratio,
        store_size_sqft,
        avg_daily_foottraffic,
        baseline_sales,
        sales_volatility,
        coefficient_of_variation,
        discount_x_duration,
        discount_x_weekend,
        price_x_discount
    FROM ML_TRAINING_DATASET_SPLIT
    WHERE dataset_split = 'TEST'
)
FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY = '"' COMPRESSION = GZIP)
HEADER = FALSE
OVERWRITE = TRUE;

SELECT 'Feature engineering complete! Data exported to S3.' AS status;
```

---

## ğŸš€ **CPG-502: MODEL TRAINING WITH SAGEMAKER**

### **Create SageMaker Training Script:**

**File: `train_promo_lift_model.py`** (Run in SageMaker Notebook or locally)

```python
"""
SageMaker Training Script: Promotional Sales Lift Prediction
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Purpose: Train XGBoost model to predict promotional sales lift
Author: You + Maria
Date: 2024-02-11
"""

import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.inputs import TrainingInput
from sagemaker.tuner import (
    IntegerParameter,
    ContinuousParameter,
    HyperparameterTuner
)
import pandas as pd
import numpy as np
from datetime import datetime

print("=" * 80)
print("ğŸ¤– SAGEMAKER TRAINING: PROMOTIONAL LIFT PREDICTION")
print("=" * 80)

# ============================================================================
# STEP 1: SETUP
# ============================================================================

# Initialize SageMaker session
sagemaker_session = sagemaker.Session()
role = get_execution_role()
region = boto3.Session().region_name

# S3 paths
bucket = 'dpa-sagemaker'
prefix = 'promo-lift-prediction'

train_path = f's3://{bucket}/{prefix}/train/'
test_path = f's3://{bucket}/{prefix}/test/'
output_path = f's3://{bucket}/{prefix}/output/'

print(f"\nğŸ“‚ S3 Paths:")
print(f"   Train: {train_path}")
print(f"   Test: {test_path}")
print(f"   Output: {output_path}")

# ============================================================================
# STEP 2: GET XGBOOST CONTAINER IMAGE
# ============================================================================

from sagemaker import image_uris

container = image_uris.retrieve(
    framework='xgboost',
    region=region,
    version='1.5-1',
    image_scope='training'
)

print(f"\nğŸ³ Container Image: {container}")

# ============================================================================
# STEP 3: CONFIGURE TRAINING JOB
# ============================================================================

# Create XGBoost estimator
xgb_estimator = sagemaker.estimator.Estimator(
    image_uri=container,
    role=role,
    instance_count=1,
    instance_type='ml.m5.2xlarge',  # 8 vCPUs, 32 GB RAM
    output_path=output_path,
    sagemaker_session=sagemaker_session,
    base_job_name='promo-lift-xgboost'
)

# Set hyperparameters
xgb_estimator.set_hyperparameters(
    objective='reg:squarederror',  # Regression
    num_round=100,                  # Number of boosting rounds
    max_depth=6,                    # Maximum tree depth
    eta=0.3,                        # Learning rate
    gamma=0,                        # Minimum loss reduction
    min_child_weight=1,             # Minimum sum of instance weight
    subsample=0.8,                  # Subsample ratio
    colsample_bytree=0.8,           # Feature subsample ratio
    verbosity=1
)

print("\nâš™ï¸  Estimator configured:")
print(f"   Instance: ml.m5.2xlarge")
print(f"   Algorithm: XGBoost 1.5-1")
print(f"   Objective: reg:squarederror")

# ============================================================================
# STEP 4: DEFINE TRAINING INPUTS
# ============================================================================

train_input = TrainingInput(
    s3_data=train_path,
    content_type='text/csv'
)

test_input = TrainingInput(
    s3_data=test_path,
    content_type='text/csv'
)

# ============================================================================
# STEP 5: TRAIN MODEL
# ============================================================================

print("\nğŸš€ Starting training job...")

xgb_estimator.fit(
    inputs={
        'train': train_input,
        'validation': test_input
    },
    wait=True,
    logs='All'
)

print("\nâœ… Training complete!")

# Get training job name
training_job_name = xgb_estimator.latest_training_job.name
print(f"Training Job: {training_job_name}")

# ============================================================================
# STEP 6: EVALUATE MODEL
# ============================================================================

print("\nğŸ“Š Model Evaluation Metrics:")

# Get training metrics from CloudWatch
sm_client = boto3.client('sagemaker')

training_job_info = sm_client.describe_training_job(
    TrainingJobName=training_job_name
)

final_metrics = training_job_info.get('FinalMetricDataList', [])

for metric in final_metrics:
    print(f"   {metric['MetricName']}: {metric['Value']:.4f}")

# ============================================================================
# STEP 7: REGISTER MODEL IN MODEL REGISTRY
# ============================================================================

print("\nğŸ“ Registering model in Model Registry...")

from sagemaker.model import Model
from sagemaker.predictor import Predictor

# Create model package group (one-time setup)
model_package_group_name = 'promo-lift-prediction-models'

try:
    sm_client.create_model_package_group(
        ModelPackageGroupName=model_package_group_name,
        ModelPackageGroupDescription='Promotional sales lift prediction models'
    )
    print(f"âœ… Created model package group: {model_package_group_name}")
except sm_client.exceptions.ValidationException:
    print(f"â„¹ï¸  Model package group already exists: {model_package_group_name}")

# Register model version
model_package = xgb_estimator.register(
    content_types=['text/csv'],
    response_types=['text/csv'],
    inference_instances=['ml.t2.medium', 'ml.m5.large'],
    transform_instances=['ml.m5.large'],
    model_package_group_name=model_package_group_name,
    approval_status='PendingManualApproval',
    description=f'Promotional lift prediction model trained on {datetime.now().strftime("%Y-%m-%d")}'
)

print(f"âœ… Model registered: {model_package.model_package_arn}")

# ============================================================================
# STEP 8: SAVE METADATA
# ============================================================================

metadata = {
    'training_job_name': training_job_name,
    'model_package_arn': model_package.model_package_arn,
    'model_package_group': model_package_group_name,
    'train_path': train_path,
    'test_path': test_path,
    'output_path': output_path,
    'container_image': container,
    'instance_type': 'ml.m5.2xlarge',
    'training_date': datetime.now().isoformat(),
    'hyperparameters': xgb_estimator.hyperparameters()
}

# Save to S3
import json

s3_client = boto3.client('s3')
s3_client.put_object(
    Bucket=bucket,
    Key=f'{prefix}/metadata/training_metadata_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json',
    Body=json.dumps(metadata, indent=2)
)

print("\nâœ… Metadata saved to S3")

print("\n" + "=" * 80)
print("ğŸ‰ MODEL TRAINING COMPLETE!")
print("=" * 80)
```

---

### **Run Training Job:**

```bash
# In SageMaker Studio or Notebook instance
python train_promo_lift_model.py
```

**Output:**

```
================================================================================
ğŸ¤– SAGEMAKER TRAINING: PROMOTIONAL LIFT PREDICTION
================================================================================

ğŸ“‚ S3 Paths:
   Train: s3://dpa-sagemaker/promo-lift-prediction/train/
   Test: s3://dpa-sagemaker/promo-lift-prediction/test/
   Output: s3://dpa-sagemaker/promo-lift-prediction/output/

ğŸ³ Container Image: 12345678.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.5-1

âš™ï¸  Estimator configured:
   Instance: ml.m5.2xlarge
   Algorithm: XGBoost 1.5-1
   Objective: reg:squarederror

ğŸš€ Starting training job...

2024-02-11 15:23:45 Starting - Starting the training job...
2024-02-11 15:24:12 Starting - Preparing the instances for training...
2024-02-11 15:24:45 Downloading - Downloading input data...
2024-02-11 15:25:23 Training - Training image download completed. Training in progress.

[0]#011train-rmse:25.3456#011validation-rmse:26.1234
[10]#011train-rmse:12.5678#011validation-rmse:13.2345
[20]#011train-rmse:8.9012#011validation-rmse:9.5678
[30]#011train-rmse:7.2345#011validation-rmse:7.8901
[40]#011train-rmse:6.5432#011validation-rmse:7.1234
[50]#011train-rmse:6.1234#011validation-rmse:6.7890
[60]#011train-rmse:5.9012#011validation-rmse:6.5432
[70]#011train-rmse:5.7890#011validation-rmse:6.4321
[80]#011train-rmse:5.7123#011validation-rmse:6.3987
[90]#011train-rmse:5.6789#011validation-rmse:6.3765
[99]#011train-rmse:5.6543#011validation-rmse:6.3654

2024-02-11 15:32:15 Uploading - Uploading generated training model
2024-02-11 15:32:45 Completed - Training job completed

âœ… Training complete!
Training Job: promo-lift-xgboost-2024-02-11-15-23-45-123

ğŸ“Š Model Evaluation Metrics:
   train:rmse: 5.6543
   validation:rmse: 6.3654
   train:mae: 4.2345
   validation:mae: 4.8765

ğŸ“ Registering model in Model Registry...
âœ… Created model package group: promo-lift-prediction-models
âœ… Model registered: arn:aws:sagemaker:us-east-1:123456789012:model-package/promo-lift-prediction-models/1

âœ… Metadata saved to S3

================================================================================
ğŸ‰ MODEL TRAINING COMPLETE!
================================================================================
```

---

## ğŸ”§ **CPG-503: HYPERPARAMETER TUNING (OPTIONAL)**

**File: `tune_promo_lift_model.py`**

```python
"""
SageMaker Hyperparameter Tuning
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Purpose: Find optimal hyperparameters for XGBoost model
"""

import sagemaker
from sagemaker import get_execution_role
from sagemaker.tuner import (
    IntegerParameter,
    ContinuousParameter,
    HyperparameterTuner
)

# Setup
sagemaker_session = sagemaker.Session()
role = get_execution_role()

# Define hyperparameter ranges
hyperparameter_ranges = {
    'max_depth': IntegerParameter(3, 10),
    'eta': ContinuousParameter(0.01, 0.3),
    'gamma': ContinuousParameter(0, 5),
    'min_child_weight': IntegerParameter(1, 10),
    'subsample': ContinuousParameter(0.5, 1.0),
    'colsample_bytree': ContinuousParameter(0.5, 1.0),
    'num_round': IntegerParameter(50, 200)
}

# Create tuner
tuner = HyperparameterTuner(
    estimator=xgb_estimator,
    objective_metric_name='validation:rmse',
    objective_type='Minimize',
    hyperparameter_ranges=hyperparameter_ranges,
    max_jobs=20,
    max_parallel_jobs=3,
    strategy='Bayesian'
)

# Start tuning
print("ğŸ” Starting hyperparameter tuning...")

tuner.fit(
    inputs={
        'train': train_input,
        'validation': test_input
    },
    wait=False
)

print(f"âœ… Tuning job started: {tuner.latest_tuning_job.name}")
print("   Monitor progress in SageMaker Console")
```

---

## ğŸš€ **CPG-504: BATCH INFERENCE PIPELINE**

**File: `batch_inference_promo_lift.py`**

```python
"""
Batch Inference Pipeline
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Purpose: Score upcoming promotions with ML predictions
"""

import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.model import Model
from sagemaker.transformer import Transformer
import pandas as pd
from datetime import datetime

print("=" * 80)
print("ğŸ“Š BATCH INFERENCE - PROMOTIONAL LIFT PREDICTION")
print("=" * 80)

# ============================================================================
# STEP 1: LOAD UPCOMING PROMOTIONS FROM SNOWFLAKE
# ============================================================================

print("\nğŸ“– Loading upcoming promotions from Snowflake...")

from snowflake.connector import connect

# Snowflake connection
conn = connect(
    user='yourname@dpa.com',
    password='your_password',
    account='abc12345',
    warehouse='COMPUTE_WH_ETL',
    database='ANALYTICS_PROD',
    schema='GOLD'
)

# Query upcoming promotions
query = """
SELECT
    promotion_id,
    discount_percentage,
    promo_duration_days,
    budget_per_day,
    promo_intensity_score,
    promo_start_month,
    promo_start_day_of_week,
    is_weekend_start,
    is_holiday_aligned,
    master_unit_price,
    cost_price,
    (master_unit_price - discounted_price) / master_unit_price * 100 AS effective_discount_pct,
    master_unit_price / cost_price AS price_to_cost_ratio,
    store_size_sqft,
    avg_daily_foottraffic,
    baseline_sales,
    sales_volatility,
    sales_volatility / NULLIF(baseline_sales, 0) AS coefficient_of_variation,
    discount_percentage * promo_duration_days AS discount_x_duration,
    discount_percentage * is_weekend_start AS discount_x_weekend,
    master_unit_price * discount_percentage AS price_x_discount
FROM PROMO_ROI_DASHBOARD
WHERE promo_status = 'Upcoming'
    AND start_date >= CURRENT_DATE()
ORDER BY start_date
LIMIT 10000
"""

upcoming_promos = pd.read_sql(query, conn)
conn.close()

print(f"âœ… Loaded {len(upcoming_promos):,} upcoming promotions")

# ============================================================================
# STEP 2: PREPARE DATA FOR INFERENCE
# ============================================================================

print("\nğŸ”§ Preparing data for inference...")

# Select features in same order as training
feature_columns = [
    'discount_percentage', 'promo_duration_days', 'budget_per_day',
    'promo_intensity_score', 'promo_start_month', 'promo_start_day_of_week',
    'is_weekend_start', 'is_holiday_aligned', 'master_unit_price',
    'cost_price', 'effective_discount_pct', 'price_to_cost_ratio',
    'store_size_sqft', 'avg_daily_foottraffic', 'baseline_sales',
    'sales_volatility', 'coefficient_of_variation', 'discount_x_duration',
    'discount_x_weekend', 'price_x_discount'
]

X_inference = upcoming_promos[feature_columns]

# Save to S3 for batch transform
bucket = 'dpa-sagemaker'
inference_input_path = f's3://{bucket}/promo-lift-prediction/inference/input/'
inference_output_path = f's3://{bucket}/promo-lift-prediction/inference/output/'

# Save as CSV
inference_file = f'inference_batch_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
X_inference.to_csv(f'/tmp/{inference_file}', header=False, index=False)

# Upload to S3
s3_client = boto3.client('s3')
s3_client.upload_file(
    f'/tmp/{inference_file}',
    bucket,
    f'promo-lift-prediction/inference/input/{inference_file}'
)

print(f"âœ… Data uploaded to S3: {inference_input_path}{inference_file}")

# ============================================================================
# STEP 3: RUN BATCH TRANSFORM JOB
# ============================================================================

print("\nğŸš€ Starting batch transform job...")

sagemaker_session = sagemaker.Session()

# Get latest approved model from registry
sm_client = boto3.client('sagemaker')

response = sm_client.list_model_packages(
    ModelPackageGroupName='promo-lift-prediction-models',
    ModelApprovalStatus='Approved',
    SortBy='CreationTime',
    SortOrder='Descending',
    MaxResults=1
)

if not response['ModelPackageSummaryList']:
    print("âŒ No approved model found. Please approve a model in Model Registry.")
    exit(1)

model_package_arn = response['ModelPackageSummaryList'][0]['ModelPackageArn']

print(f"ğŸ“¦ Using model: {model_package_arn}")

# Create model from package
model = Model(
    image_uri=None,  # Will be inferred from model package
    model_data=None,  # Will be inferred from model package
    role=get_execution_role(),
    sagemaker_session=sagemaker_session,
    name=f'promo-lift-model-{datetime.now().strftime("%Y%m%d%H%M%S")}',
    predictor_cls=sagemaker.predictor.Predictor,
    model_package_arn=model_package_arn
)

# Create transformer
transformer = Transformer(
    model_name=model.name,
    instance_count=1,
    instance_type='ml.m5.xlarge',
    output_path=inference_output_path,
    accept='text/csv',
    assemble_with='Line',
    sagemaker_session=sagemaker_session
)

# Run batch transform
transformer.transform(
    data=f'{inference_input_path}{inference_file}',
    content_type='text/csv',
    split_type='Line',
    wait=True,
    logs=True
)

print("\nâœ… Batch transform complete!")
print(f"   Output: {inference_output_path}")

# ============================================================================
# STEP 4: LOAD PREDICTIONS AND SAVE TO SNOWFLAKE
# ============================================================================

print("\nğŸ“¥ Loading predictions...")

# Download predictions from S3
prediction_file = inference_file + '.out'
s3_client.download_file(
    bucket,
    f'promo-lift-prediction/inference/output/{prediction_file}',
    f'/tmp/{prediction_file}'
)

# Read predictions
predictions = pd.read_csv(f'/tmp/{prediction_file}', header=None, names=['predicted_lift_pct'])

# Combine with original data
results = pd.concat([
    upcoming_promos[['promotion_id']].reset_index(drop=True),
    predictions.reset_index(drop=True)
], axis=1)

results['prediction_timestamp'] = datetime.now()
results['model_version'] = model_package_arn.split('/')[-1]

print(f"âœ… Generated {len(results):,} predictions")

# Save to Snowflake
print("\nğŸ’¾ Saving predictions to Snowflake...")

conn = connect(
    user='yourname@dpa.com',
    password='your_password',
    account='abc12345',
    warehouse='COMPUTE_WH_ETL',
    database='ANALYTICS_PROD',
    schema='ML_PREDICTIONS'
)

cursor = conn.cursor()

# Create table if not exists
cursor.execute("""
    CREATE TABLE IF NOT EXISTS PROMO_LIFT_PREDICTIONS (
        promotion_id VARCHAR(100),
        predicted_lift_pct FLOAT,
        prediction_timestamp TIMESTAMP_NTZ,
        model_version VARCHAR(100)
    )
""")

# Insert predictions
from snowflake.connector.pandas_tools import write_pandas

write_pandas(
    conn,
    results,
    table_name='PROMO_LIFT_PREDICTIONS',
    schema='ML_PREDICTIONS',
    database='ANALYTICS_PROD'
)

conn.close()

print("âœ… Predictions saved to Snowflake!")

# ============================================================================
# SUMMARY
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š BATCH INFERENCE SUMMARY")
print("=" * 80)
print(f"Promotions Scored: {len(results):,}")
print(f"Model Version: {results['model_version'].iloc[0]}")
print(f"\nPrediction Statistics:")
print(f"  Mean Predicted Lift: {results['predicted_lift_pct'].mean():.2f}%")
print(f"  Min Predicted Lift: {results['predicted_lift_pct'].min():.2f}%")
print(f"  Max Predicted Lift: {results['predicted_lift_pct'].max():.2f}%")
print(f"  Median Predicted Lift: {results['predicted_lift_pct'].median():.2f}%")
print("=" * 80)
```

---

Due to length constraints, I'll continue with Real-time Endpoint Deployment in the next response. Should I continue with:

1. **Real-time Endpoint (API for live predictions)**
2. **Model Monitoring (SageMaker Model Monitor)**
3. **Complete MLOps Pipeline (SageMaker Pipelines)**

Which one would you like to see next? ğŸš€
