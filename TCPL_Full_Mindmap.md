# ğŸ§  TCPL Ã— Decision Point Analytics
## Complete End-to-End Architecture Mindmap
### "What Actually Happens" â€” Your Field Guide for Day 1

---

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚     TCPL (Tata Consumer Products)        â”‚
                        â”‚         CLIENT â€” Your Workplace          â”‚
                        â”‚                                          â”‚
                        â”‚  Tata Tea Â· Tetley Â· Himalayan Water     â”‚
                        â”‚  Tata Salt Â· Tata Sampann Â· Eight O'Clockâ”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                        What does TCPL need from data?
                                           â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â–¼                           â–¼                        â–¼
     How much product          Which promotions         Where is my
     will sell next            actually worked?         inventory low?
     3 months?                 (Trade Promo ROI)        (Stockouts?)
     (Demand Forecast)         (MMM â€” Mkt Mix Model)    (Supply Chain)
```

---

## ğŸŒ THE BIG PICTURE â€” One Line Per Layer

```
REAL WORLD                 â†’    SAP / ERP / Nielsen APIs      (where data is born)
         â”‚
         â–¼
DATA LANDS IN AWS S3       â†’    Raw files arrive daily        (data lake = storage)
         â”‚
         â–¼
AIRFLOW WAKES UP           â†’    Apache Airflow (MWAA)         (boss â€” tells everyone what to do)
         â”‚
         â–¼
GLUE CLEANS THE DATA       â†’    AWS Glue PySpark              (factory â€” heavy lifting)
         â”‚
         â–¼
LAMBDA CHECKS THE DATA     â†’    AWS Lambda                    (security guard â€” validates)
         â”‚
         â–¼
DATA GOES TO SNOWFLAKE     â†’    Snowflake DW                  (warehouse â€” final clean home)
         â”‚
         â–¼
ML MODEL RUNS              â†’    AWS SageMaker                 (brain â€” forecasts future)
         â”‚
         â–¼
PREDICTIONS IN SNOWFLAKE   â†’    ML outputs stored             (forecast available to business)
         â”‚
         â–¼
BUSINESS SEES DASHBOARDS   â†’    Tableau / PowerBI             (TCPL managers see charts)
```

---

## ğŸ“¦ LAYER 1 â€” DATA SOURCES
### "Where does data come from at TCPL?"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA SOURCES                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 â”‚                  â”‚               â”‚            â”‚
â”‚   SAP S4/HANA   â”‚  Nielsen / IRI   â”‚  Trade Promo  â”‚  External  â”‚
â”‚                 â”‚                  â”‚  Portal       â”‚  Sources   â”‚
â”‚ - Sales Orders  â”‚ - Market Share   â”‚               â”‚            â”‚
â”‚ - Inventory     â”‚ - Competitor     â”‚ - Schemes     â”‚ - Weather  â”‚
â”‚ - Purchase Ord  â”‚   pricing        â”‚ - Discounts   â”‚ - Holidays â”‚
â”‚ - Finance data  â”‚ - Volume data    â”‚ - BTL spends  â”‚ - GDP data â”‚
â”‚ - Customer masterâ”‚ - Distribution  â”‚ - Retailer    â”‚            â”‚
â”‚ - SKU master    â”‚   reach          â”‚   coverage    â”‚            â”‚
â”‚                 â”‚                  â”‚               â”‚            â”‚
â”‚ FORMAT: Parquet â”‚ FORMAT: CSV      â”‚ FORMAT: Excel â”‚ FORMAT: APIâ”‚
â”‚ FREQ: Daily     â”‚ FREQ: Weekly     â”‚ FREQ: Monthly â”‚ FREQ: Dailyâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚  How does it get to AWS?
         â”‚
         â”œâ”€â”€ Batch Export â†’ SFTP â†’ S3 (SAP)
         â”œâ”€â”€ API Pull â†’ Lambda â†’ S3 (Nielsen)
         â””â”€â”€ Manual Upload â†’ S3 (Trade Promos)
```

---

## ğŸ—„ï¸ LAYER 2 â€” AWS S3 DATA LAKE
### "The big hard disk in the cloud"

```
s3://tcpl-datalake/
â”‚
â”œâ”€â”€ ğŸ“‚ raw/                     â† UNTOUCHED original files (never modify)
â”‚   â”œâ”€â”€ sap/
â”‚   â”‚   â”œâ”€â”€ sales_orders/
â”‚   â”‚   â”‚   â””â”€â”€ year=2024/month=01/day=15/    â† PARTITIONED by date
â”‚   â”‚   â”‚       â””â”€â”€ sales_20240115.parquet
â”‚   â”‚   â”œâ”€â”€ inventory/
â”‚   â”‚   â””â”€â”€ purchase_orders/
â”‚   â”œâ”€â”€ nielsen/
â”‚   â”‚   â””â”€â”€ market_share/week=2024-W03/
â”‚   â””â”€â”€ trade_promotions/
â”‚       â””â”€â”€ promo_calendar/
â”‚
â”œâ”€â”€ ğŸ“‚ curated/                 â† CLEANED data after Glue ETL
â”‚   â”œâ”€â”€ sales/                  â† Deduplicated, validated, enriched
â”‚   â”œâ”€â”€ inventory/
â”‚   â”œâ”€â”€ market_data/
â”‚   â””â”€â”€ promotions/
â”‚
â”œâ”€â”€ ğŸ“‚ ml-features/             â† INPUT to SageMaker (feature engineered)
â”‚   â”œâ”€â”€ demand_forecast/
â”‚   â”‚   â””â”€â”€ lag_features/
â”‚   â””â”€â”€ mmm/
â”‚
â”œâ”€â”€ ğŸ“‚ model-artifacts/         â† OUTPUT from SageMaker (trained models)
â”‚   â”œâ”€â”€ demand_forecast/
â”‚   â”‚   â””â”€â”€ v1.2/model.tar.gz
â”‚   â””â”€â”€ predictions/            â† Forecast CSV files
â”‚       â””â”€â”€ 2024-01-15/
â”‚
â””â”€â”€ ğŸ“‚ scripts/                 â† Glue + SageMaker code lives here
    â”œâ”€â”€ glue/
    â””â”€â”€ sagemaker/

KEY RULE: raw/ = read only forever. Never overwrite raw data.
```

---

## ğŸ§  LAYER 3 â€” APACHE AIRFLOW (MWAA)
### "The Boss. Does nothing itself. Tells everyone else when to run."

```
WHAT IS AIRFLOW?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Think of it like a PROJECT MANAGER.
  It does NOT do any data work.
  It just says: "Hey Glue, start now." / "Hey Lambda, check this."
  If something fails â†’ it retries â†’ if still fails â†’ sends alert

  Runs on AWS MWAA (Managed Workflows for Apache Airflow)
  You access it via a web URL (Airflow UI)

WHAT IS A DAG?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  DAG = Directed Acyclic Graph = a PIPELINE defined in Python
  Each box in the pipeline = a TASK
  Tasks have ORDER and DEPENDENCIES

  DAG File Location: s3://tcpl-datalake/dags/

THE MAIN DAG: tcpl_cpg_full_pipeline
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Schedule: Every day at 2:00 AM IST
  
  TASK 1 â”€â”€â–º TASK 2 â”€â”€â–º TASK 3 â”€â”€â–º TASK 4 â”€â”€â–º TASK 5
  (Check)    (ETL)      (Validate)  (Snowflake) (ML)


FULL DAG FLOW:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  [START 2AM]
       â”‚
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  TASK 1: check_source_files  â”‚  â† Lambda checks S3 for today's files
  â”‚  (Lambda)                    â”‚    "Did SAP files arrive? Nielsen files?"
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Files Present? â”‚
        â”‚                  â”‚
       YES                NO
        â”‚                  â”‚
        â–¼                  â–¼
  Continue             STOP + Alert
                       Slack + Email

       YES path continues...
        â”‚
        â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  TASK 2,3,4: Glue ETL Jobs (run in PARALLEL)     â”‚
  â”‚                                                  â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚  â”‚ SAP Sales    â”‚  â”‚  Inventory   â”‚  â”‚Nielsen â”‚ â”‚
  â”‚  â”‚ ETL Job      â”‚  â”‚  ETL Job     â”‚  â”‚ETL Job â”‚ â”‚
  â”‚  â”‚ (Glue)       â”‚  â”‚  (Glue)      â”‚  â”‚(Glue)  â”‚ â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
  â”‚  All 3 run at the SAME TIME (parallel)           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â”‚ All 3 must finish before next step
                        â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  TASK 5: validate_data_quality       â”‚ â† Lambda checks the cleaned data
  â”‚  (Lambda)                            â”‚   "Are null rates ok? Row count ok?"
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Quality Passed?   â”‚
          YES                  NO
           â”‚                    â”‚
           â–¼                    â–¼
     Continue             STOP + Alert
                          "Bad data! Don't load!"

          YES path continues...
           â”‚
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  TASK 6: load_to_snowflake           â”‚ â† Run SQL in Snowflake
  â”‚  (SnowflakeOperator)                 â”‚   Load curated data to DW
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  TASK 7: glue_feature_engineering    â”‚ â† Build ML features from clean data
  â”‚  (Glue)                              â”‚   Lag values, rolling averages, etc.
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  TASK 8: sagemaker_demand_forecast   â”‚ â† Train + predict demand
  â”‚  (SageMaker Pipeline)                â”‚   Takes ~30-60 min
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  TASK 9: store_predictions           â”‚ â† Lambda loads predictions â†’ Snowflake
  â”‚  (Lambda)                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  TASK 10: refresh_data_marts         â”‚ â† Snowflake stored proc
  â”‚  (SnowflakeOperator)                 â”‚   Refresh all BI views
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  TASK 11: pipeline_success_alert     â”‚ â† Slack: âœ… Pipeline done!
  â”‚  (Lambda)                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  [END ~6AM] â€” Dashboards ready for business by morning

WHAT YOU'LL DO IN AIRFLOW UI:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â†’ Monitor DAG runs (green = success, red = failed)
  â†’ Re-trigger failed tasks manually
  â†’ Check logs of each task
  â†’ See how long each task took
```

---

## âš™ï¸ LAYER 4 â€” AWS GLUE
### "The Factory. Does all heavy data lifting."

```
WHAT IS GLUE?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Serverless Spark (PySpark) on AWS.
  You write Python/PySpark code.
  AWS handles the servers â€” you just pay per DPU (compute unit).
  
  Think of it as: "Run PySpark without managing clusters"

GLUE JOB 1: tcpl-glue-sap-sales-transform
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  INPUT  â†’ s3://tcpl-datalake/raw/sap/sales_orders/year=2024/month=01/day=15/
  OUTPUT â†’ s3://tcpl-datalake/curated/sales/
         â†’ Snowflake table: CURATED.SALES_ORDERS

  WHAT IT DOES STEP BY STEP:
  
  Step 1: READ raw parquet from S3
           spark.read.parquet("s3://tcpl-datalake/raw/sap/...")
  
  Step 2: VALIDATE schema
           Check required columns exist
           (order_id, sku_code, quantity, net_revenue, etc.)
  
  Step 3: CLEAN
           - Remove duplicates (dropDuplicates on order_id)
           - Fix date formats (to_date)
           - Uppercase SKU codes (TRIM + UPPER)
           - Filter out negative quantities
           - Filter out zero revenue orders
  
  Step 4: ENRICH
           - Calculate revenue_per_unit = net_revenue / quantity
           - Add year, month, quarter columns
           - Flag modern_trade vs general_trade
           - Add processing_timestamp
  
  Step 5: ANOMALY DETECTION
           - 4-week rolling average per SKU per region
           - If revenue > 3x rolling avg â†’ flag as anomaly
           - Business team investigates anomalies manually
  
  Step 6: WRITE to S3 curated (partitioned by year/month)
  
  Step 7: WRITE to Snowflake via JDBC connector


GLUE JOB 2: tcpl-glue-inventory-transform
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Similar process for inventory data
  Calculates: Days of Supply, Stockout flags, Overstock flags


GLUE JOB 3: tcpl-glue-feature-engineering  (ML prep)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  INPUT  â†’ s3://tcpl-datalake/curated/sales/ + promotions/
  OUTPUT â†’ s3://tcpl-datalake/ml-features/demand_forecast/

  WHAT IT DOES:
  
  Aggregates to WEEKLY level per SKU per Region
  
  Creates LAG FEATURES:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  lag_1w_units  = sales from 1 week ago                   â”‚
  â”‚  lag_2w_units  = sales from 2 weeks ago                  â”‚
  â”‚  lag_4w_units  = sales from 4 weeks ago (1 month)        â”‚
  â”‚  lag_8w_units  = sales from 8 weeks ago (2 months)       â”‚
  â”‚  yoy_change    = compared to same week last year          â”‚
  â”‚  rolling_4w_avg = average of last 4 weeks                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  Creates SEASONALITY FEATURES:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  is_festive_season  (Diwali, Holi, Puja weeks)           â”‚
  â”‚  is_summer          (April-June)                         â”‚
  â”‚  week_of_year, month, quarter                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  Joins PROMOTION data:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  promo_discount_pct = % discount during that week        â”‚
  â”‚  is_promoted = 1 if promotion was running                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

HOW TO MONITOR GLUE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â†’ AWS Console â†’ Glue â†’ Jobs â†’ tcpl-glue-xxx
  â†’ See run history, duration, DPU used
  â†’ CloudWatch logs for errors
  â†’ Airflow task logs also show Glue output
```

---

## ğŸ”” LAYER 5 â€” AWS LAMBDA
### "The Lightweight Helper. Quick tasks only."

```
WHAT IS LAMBDA?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Serverless Python function.
  Runs in milliseconds to minutes.
  No servers to manage.
  Triggered by Airflow, S3 events, SNS, or schedule.

  Rule of thumb: If it takes < 15 minutes â†’ Lambda
                 If it takes > 15 minutes â†’ Glue

LAMBDA 1: tcpl-preflight-source-check
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  WHEN: First task in Airflow DAG (2:00 AM)
  
  DOES:
    - Checks S3 if today's SAP file landed
    - Checks S3 if Nielsen weekly file is present
    - If any file missing â†’ send SNS alert â†’ email + Slack
    - Returns: { "all_sources_available": true/false }
  
  WHY IMPORTANT:
    Without this check, Glue would run on empty folders
    and produce zero records with no error â€” silent failure!
    This catches it early.


LAMBDA 2: tcpl-data-quality-validator
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  WHEN: After all Glue ETL jobs finish
  
  DOES:
    - Checks NULL rate (should be < 5%)
    - Checks ROW COUNT (should be > 1000 rows)
    - Checks DUPLICATE rate (should be < 1%)
    - Checks SCHEMA is correct
    - Emits CloudWatch metrics (visible on dashboards)
    - Returns: { "quality_passed": true/false }
  
  AIRFLOW BRANCHES based on result:
    true  â†’ continue to Snowflake load
    false â†’ stop pipeline + alert team


LAMBDA 3: tcpl-slack-alerter
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  WHEN: On failure OR on success
  
  DOES:
    - Sends formatted Slack message to #tcpl-data-alerts
    - Includes: date, failed task, error message
    - Severity levels: HIGH (red) / MEDIUM (yellow) / INFO (green)
  
  Example Slack message:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ âš ï¸ HIGH ALERT â€” TCPL Pipeline               â”‚
  â”‚ Date: 2024-01-15                            â”‚
  â”‚ Failed Task: glue_sap_sales_etl             â”‚
  â”‚ Error: Row count = 0 (expected > 5000)      â”‚
  â”‚ Action: Check SAP export for today          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


LAMBDA 4: tcpl-predictions-loader
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  WHEN: After SageMaker batch predictions are ready
  
  DOES:
    - Reads prediction CSV from S3
    - MERGES into Snowflake (UPSERT â€” no duplicates)
    - Adds model_version and load_date
```

---

## ğŸ¤– LAYER 6 â€” AWS SAGEMAKER
### "The ML Lab. Trains models and predicts the future."

```
WHAT IS SAGEMAKER?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  AWS managed ML platform.
  You define a PIPELINE (series of ML steps).
  AWS handles compute â€” spins up ml.m5.xlarge etc.
  
  At TCPL, SageMaker runs DEMAND FORECASTING.

USE CASE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€
  QUESTION: "How many units of Tata Tea Premium 500g
             will sell in Maharashtra in the next 90 days?"
  
  ANSWER: SageMaker XGBoost model gives weekly predictions
          with confidence intervals (lower/upper bounds)

THE SAGEMAKER PIPELINE STEPS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  STEP 1: PREPROCESSING
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Input:  s3://tcpl-datalake/ml-features/demand_forecast/
  Output: train (70%) / validation (20%) / test (10%) splits
  Code:   SKLearnProcessor runs preprocess_demand.py
  
  What happens:
  - Normalize features (scale 0 to 1)
  - Handle remaining nulls (fill with median)
  - Encode categorical variables (brand, region â†’ numbers)
  - Create final feature matrix for XGBoost


  STEP 2: TRAINING
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Algorithm: XGBoost (gradient boosted trees)
  Instance:  ml.m5.2xlarge (8 cores, 32GB RAM)
  
  Key Hyperparameters:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  max_depth       = 8             â”‚ â† tree depth
  â”‚  eta             = 0.1           â”‚ â† learning rate
  â”‚  num_round       = 500           â”‚ â† training iterations
  â”‚  early_stopping  = 20            â”‚ â† stop if no improvement
  â”‚  objective       = reg:squarederror â”‚ â† regression task
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  Output: model.tar.gz saved to S3


  STEP 3: EVALUATION
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Runs on TEST data (never seen during training)
  
  Metric: MAPE (Mean Absolute Percentage Error)
  
  MAPE = average of |actual - predicted| / actual Ã— 100
  
  Example:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Actual: 1000 units                                â”‚
  â”‚  Predicted: 950 units                              â”‚
  â”‚  Error: |1000-950|/1000 = 5% MAPE                 â”‚
  â”‚                                                    â”‚
  â”‚  Target: MAPE < 15% (if >= 15% â†’ model rejected)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


  STEP 4: QUALITY GATE
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  IF MAPE < 15% â†’ register model + proceed to predictions
  IF MAPE >= 15% â†’ FAIL STEP fires â†’ pipeline stops
                    Alert sent â†’ data science team investigates


  STEP 5: MODEL REGISTRY
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Good models get registered in SageMaker Model Registry
  Status: "PendingManualApproval"
  Data Science lead approves â†’ moves to "Approved"
  Only Approved models go to production


  STEP 6: BATCH TRANSFORM (Predictions)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Runs the approved model on ALL SKU-Region combinations
  Generates 90-day weekly forecast for each
  
  Output format (CSV):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ sku_code â”‚ region â”‚ week       â”‚ predicted â”‚ lower â”‚ upperâ”‚
  â”‚ TTP-500  â”‚ MH     â”‚ 2024-01-22 â”‚ 15200     â”‚ 13800 â”‚16600 â”‚
  â”‚ TTP-500  â”‚ MH     â”‚ 2024-01-29 â”‚ 15800     â”‚ 14200 â”‚17400 â”‚
  â”‚ TTP-500  â”‚ MH     â”‚ 2024-02-05 â”‚ 14900     â”‚ 13500 â”‚16300 â”‚
  â”‚ ...      â”‚ ...    â”‚ ...        â”‚ ...       â”‚ ...   â”‚ ...  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


  STEP 7: MODEL MONITOR (ongoing)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Watches LIVE predictions every hour
  Detects DATA DRIFT (is today's data very different from training data?)
  Alerts if drift detected â†’ model may need retraining
```

---

## ğŸ¢ LAYER 7 â€” SNOWFLAKE DATA WAREHOUSE
### "The Final Clean Home. Where business users live."

```
WHAT IS SNOWFLAKE?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Cloud Data Warehouse.
  SQL-based (you write SELECT, JOIN, GROUP BY etc.)
  Separates storage and compute (scales independently).
  Very fast for analytics queries.

SNOWFLAKE ARCHITECTURE AT TCPL:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  DATABASE: TCPL_CPG_DB
  â”‚
  â”œâ”€â”€ SCHEMA: RAW           â† Direct landing from Glue (staging)
  â”œâ”€â”€ SCHEMA: CURATED        â† Cleaned, validated data
  â”œâ”€â”€ SCHEMA: DW             â† Star schema (facts + dimensions)
  â”œâ”€â”€ SCHEMA: MARTS          â† Subject-area views for BI teams
  â”œâ”€â”€ SCHEMA: ML_OUTPUTS     â† SageMaker predictions
  â””â”€â”€ SCHEMA: MONITORING     â† Pipeline health logs

WAREHOUSES (compute clusters):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  TCPL_LOAD_WH        â† Glue + Lambda loads data   â”‚
  â”‚  (MEDIUM, auto-suspend 2 min)                      â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  TCPL_TRANSFORM_WH   â† dbt models + stored procs  â”‚
  â”‚  (LARGE, auto-suspend 5 min)                       â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  TCPL_ANALYTICS_WH   â† Tableau / analyst queries  â”‚
  â”‚  (SMALL, auto-suspend 1 min)                       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  WHY SEPARATE WAREHOUSES?
  So analyst queries don't slow down ETL loads
  And ETL loads don't slow down dashboards

STAR SCHEMA:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

         DIM_DATE
             â”‚
             â”‚
  DIM_SKU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FACT_SALES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DIM_CUSTOMER
  (product info)  â”‚    (numbers: qty,         (who bought)
                  â”‚     revenue, margin)
             DIM_GEOGRAPHY
             (where: state, zone)

  DIM = dimension table = WHO, WHAT, WHERE, WHEN (descriptive)
  FACT = fact table = the actual NUMBERS (quantity, revenue)

  Example query the business runs:
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  "Show me Tata Tea Premium monthly revenue in Maharashtra
   for last 12 months compared to forecast"

  SELECT
    d.month_name,
    sk.sku_name,
    g.state,
    SUM(f.net_revenue) AS actual_revenue,
    AVG(p.predicted_units * sk.mrp) AS forecast_revenue
  FROM FACT_SALES f
  JOIN DIM_DATE d ON f.date_key = d.date_key
  JOIN DIM_SKU sk ON f.sku_key = sk.sku_key
  JOIN DIM_GEOGRAPHY g ON f.geo_key = g.geo_key
  JOIN ML_OUTPUTS.DEMAND_FORECAST_PREDICTIONS p
    ON p.sku_code = sk.sku_code AND p.region_code = g.region_code
  WHERE sk.sku_name = 'Tata Tea Premium 500g'
    AND g.state = 'Maharashtra'
    AND d.fiscal_year = 2024
  GROUP BY 1,2,3
  ORDER BY d.month;

DATA FLOW INTO SNOWFLAKE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Glue     â”€â”€â–º writes via Snowflake Spark connector
  Lambda   â”€â”€â–º writes via snowflake-connector-python (MERGE)
  Airflow  â”€â”€â–º runs SQL via SnowflakeOperator
  SageMakerâ”€â”€â–º predictions via Lambda â†’ Snowflake MERGE
```

---

## ğŸ“Š LAYER 8 â€” BI / DASHBOARDS
### "What TCPL business managers actually see every morning"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TABLEAU / POWER BI                         â”‚
â”‚               (connects to Snowflake MARTS schema)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â–¼                  â–¼                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Sales          â”‚ â”‚  Demand Forecast  â”‚ â”‚  Trade Promo    â”‚
    â”‚  Performance    â”‚ â”‚  Dashboard        â”‚ â”‚  Effectiveness  â”‚
    â”‚  Dashboard      â”‚ â”‚                  â”‚ â”‚  Dashboard      â”‚
    â”‚                 â”‚ â”‚ - Next 90 day     â”‚ â”‚                 â”‚
    â”‚ - Daily sales   â”‚ â”‚   units forecast  â”‚ â”‚ - Which promos  â”‚
    â”‚ - Brand-wise    â”‚ â”‚ - Confidence      â”‚ â”‚   drove sales?  â”‚
    â”‚ - Region-wise   â”‚ â”‚   intervals       â”‚ â”‚ - ROI per       â”‚
    â”‚ - Channel-wise  â”‚ â”‚ - Actual vs       â”‚ â”‚   scheme        â”‚
    â”‚ - YoY growth    â”‚ â”‚   forecast MAPE   â”‚ â”‚ - Region-wise   â”‚
    â”‚ - Anomaly flags â”‚ â”‚ - Stockout risk   â”‚ â”‚   effectiveness â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    WHO USES WHAT:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Sales Head          â†’ Sales Performance Dashboard
    Demand Planner      â†’ Forecast Dashboard (daily)
    Trade Marketing     â†’ Promo Effectiveness Dashboard
    Supply Chain Team   â†’ Inventory + Stockout Dashboard
    CFO / Leadership    â†’ Executive Summary Dashboard
```

---

## ğŸš¨ LAYER 9 â€” ERROR HANDLING & MONITORING
### "What happens when things break (they will)"

```
MONITORING STACK:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Airflow UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º See task status (green/red)
                                 Re-run failed tasks
                                 View logs per task

  CloudWatch Dashboards â”€â”€â”€â”€â”€â”€â”€â–º Pipeline KPIs in real-time
                                 Glue DPU usage
                                 Lambda errors
                                 SageMaker training metrics

  CloudWatch Alarms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Trigger SNS when:
                                 - Null rate > 5%
                                 - Row count = 0
                                 - Glue job runs > 2 hours
                                 - Model drift detected

  SNS â†’ Slack/Email â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º #tcpl-data-alerts Slack channel
                                 data-ops@decisionpoint.ai

COMMON FAILURES YOU'LL FACE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  PROBLEM 1: SAP file didn't arrive on time
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Symptom: Pre-flight Lambda fails
  Slack:   "SAP sales file missing for 2024-01-15"
  Action:  Check with TCPL SAP team â†’ manual trigger once file arrives
  Fix:     Airflow â†’ re-trigger from Task 1

  PROBLEM 2: Glue job fails with "NullPointerException"
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Symptom: glue_sap_sales_etl task turns red
  Cause:   Schema change in SAP export (new column added/removed)
  Action:  Check Glue CloudWatch logs
           Update schema validation in Glue script
  Fix:     Redeploy Glue script â†’ re-trigger from Task 2

  PROBLEM 3: Row count drops 90% suddenly
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Symptom: Data quality Lambda flags failure
  Cause:   SAP partial extract / network issue during export
  Action:  Check raw S3 file size â†’ compare to yesterday
           Contact SAP team for re-extract
  Fix:     Once re-extract done â†’ re-trigger full pipeline

  PROBLEM 4: SageMaker MAPE > 15%
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Symptom: FailStep fires in SageMaker pipeline
  Cause:   Model drift â€” real data changed from training data
  Action:  Retrain with more recent data
           Check if major business event happened (new promo, COVID etc.)
  Fix:     Data Science team retrains â†’ update model version
```

---

## ğŸ—“ï¸ DAILY TIMELINE â€” What Happens Every Day

```
TIME (IST)    EVENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 11:00 PM     SAP batch export job runs at TCPL
              Data extracted from SAP S4/HANA â†’ SFTP server

 12:00 AM     SFTP â†’ S3 transfer job runs
              Files land in s3://tcpl-datalake/raw/sap/

 01:00 AM     Nielsen weekly data arrives (Mondays only)
              Trade promo file arrives (monthly)

 02:00 AM  â˜…  AIRFLOW DAG STARTS
              Task 1: Pre-flight check (Lambda)

 02:05 AM     Glue ETL jobs start in parallel
              Sales + Inventory + Market ETL

 03:30 AM     Glue jobs finish
              Task 5: Data quality Lambda runs

 03:35 AM     Snowflake load begins (Task 6)
              Stored procedure: LOAD_CURATED_DATA

 04:00 AM     Feature engineering Glue job starts (Task 7)

 04:30 AM     SageMaker demand forecast pipeline starts (Task 8)

 05:45 AM     SageMaker finishes predictions
              Lambda loads to Snowflake (Task 9)

 06:00 AM  â˜…  Snowflake marts refreshed (Task 10)
              Dashboards ready

 06:05 AM     Slack: âœ… "TCPL pipeline completed for 2024-01-15"

 09:00 AM     Business users log into Tableau / PowerBI
              See fresh data from last night's pipeline
```

---

## ğŸ“ WHERE CODE LIVES â€” Repository Structure

```
tcpl-cpg-platform/
â”‚
â”œâ”€â”€ ğŸ“‚ dags/                           â† Airflow DAGs (Python)
â”‚   â”œâ”€â”€ tcpl_cpg_full_pipeline_dag.py  â† Main daily pipeline
â”‚   â”œâ”€â”€ tcpl_weekly_nielsen_dag.py     â† Weekly Nielsen processing
â”‚   â””â”€â”€ tcpl_monthly_mmm_dag.py        â† Monthly MMM model
â”‚
â”œâ”€â”€ ğŸ“‚ glue/                           â† Glue PySpark scripts
â”‚   â”œâ”€â”€ sap_sales_transform.py
â”‚   â”œâ”€â”€ inventory_transform.py
â”‚   â”œâ”€â”€ nielsen_transform.py
â”‚   â””â”€â”€ feature_engineering.py
â”‚
â”œâ”€â”€ ğŸ“‚ lambdas/                        â† Lambda functions
â”‚   â”œâ”€â”€ preflight_check/
â”‚   â”‚   â””â”€â”€ handler.py
â”‚   â”œâ”€â”€ data_quality/
â”‚   â”‚   â””â”€â”€ handler.py
â”‚   â”œâ”€â”€ predictions_loader/
â”‚   â”‚   â””â”€â”€ handler.py
â”‚   â””â”€â”€ slack_alerter/
â”‚       â””â”€â”€ handler.py
â”‚
â”œâ”€â”€ ğŸ“‚ sagemaker/                      â† ML code
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â””â”€â”€ demand_forecast_pipeline.py
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ preprocess_demand.py
â”‚   â”‚   â””â”€â”€ evaluate_demand.py
â”‚   â””â”€â”€ notebooks/
â”‚       â””â”€â”€ demand_forecast_eda.ipynb
â”‚
â”œâ”€â”€ ğŸ“‚ snowflake/                      â† SQL
â”‚   â”œâ”€â”€ ddl/
â”‚   â”‚   â”œâ”€â”€ create_dim_tables.sql
â”‚   â”‚   â”œâ”€â”€ create_fact_tables.sql
â”‚   â”‚   â””â”€â”€ create_ml_outputs.sql
â”‚   â”œâ”€â”€ procedures/
â”‚   â”‚   â”œâ”€â”€ load_curated_data.sql
â”‚   â”‚   â””â”€â”€ refresh_marts.sql
â”‚   â””â”€â”€ views/
â”‚       â”œâ”€â”€ sales_performance_mart.sql
â”‚       â””â”€â”€ forecast_accuracy_mart.sql
â”‚
â”œâ”€â”€ ğŸ“‚ terraform/                      â† Infrastructure as Code
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ s3.tf
â”‚   â”œâ”€â”€ glue.tf
â”‚   â”œâ”€â”€ lambda.tf
â”‚   â”œâ”€â”€ mwaa.tf
â”‚   â””â”€â”€ variables.tf
â”‚
â””â”€â”€ ğŸ“‚ tests/                          â† Unit + integration tests
    â”œâ”€â”€ test_glue_transforms.py
    â””â”€â”€ test_lambda_handlers.py
```

---

## ğŸ” SECURITY & ACCESS

```
WHO ACCESSES WHAT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Decision Point Engineers â”€â”€â–º AWS Console (Glue, Lambda, S3, SageMaker)
                               Airflow UI (MWAA)
                               Snowflake (all schemas)
                               Git repository
  
  TCPL Data Team     â”€â”€â”€â”€â”€â”€â–º Airflow UI (monitor only)
                               Snowflake (MARTS schema only)
                               Tableau / PowerBI
  
  TCPL Business Users â”€â”€â”€â”€â”€â–º Tableau / PowerBI only
                               NO direct Snowflake access

  IAM ROLES:
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TCPLGlueRole      â†’ Read S3 raw, Write S3 curated, Write Snowflake
  TCPLLambdaRole    â†’ Read/Write S3, Invoke SNS, Write CloudWatch
  TCPLSageMakerRole â†’ Read S3 features, Write S3 artifacts
  TCPLMWAARole      â†’ Trigger Glue, Invoke Lambda, Trigger SageMaker

  SECRETS MANAGER:
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  tcpl-snowflake-credentials  â† Snowflake user/password
  tcpl-nielsen-api-key        â† Nielsen API key
  tcpl-sap-sftp-key           â† SAP SFTP credentials
  (Never hardcode credentials in code!)
```

---

## ğŸ’¡ KEY CONCEPTS TO REMEMBER

```
CONCEPT               WHAT IT MEANS IN SIMPLE WORDS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Partitioning     â†’   Organizing S3 files by date folder
                     year=2024/month=01/day=15/
                     Makes queries 100x faster

DPU              â†’   Glue compute unit (like 1 worker)
                     More DPUs = faster job = more cost
                     Typical TCPL jobs: 4-10 DPUs

MAPE             â†’   How wrong is the forecast in %?
                     MAPE = 10% means predictions are 10% off on avg
                     Target: < 15% at TCPL

DAG              â†’   Your pipeline in Airflow (Python file)
                     Has tasks + order + schedule

MERGE (UPSERT)   â†’   INSERT if new, UPDATE if already exists
                     Used for loading predictions to Snowflake
                     Prevents duplicate records

Data Drift       â†’   When real-world data changes pattern
                     vs what the model was trained on
                     SageMaker Monitor detects this

Star Schema      â†’   Data warehouse design pattern
                     FACT table in center (numbers)
                     DIM tables around it (descriptions)

Auto-suspend     â†’   Snowflake WH shuts down when idle
                     Saves cost â€” resumes automatically on query

Feature Store    â†’   ML-ready dataset with engineered features
                     Lives in s3://tcpl-datalake/ml-features/
```

---

## ğŸš€ YOUR FIRST WEEK AT TCPL â€” What To Do

```
DAY 1 â€” ORIENTATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… Get AWS Console access (ap-south-1 region)
  âœ… Get Airflow UI URL and login
  âœ… Get Snowflake account URL and login
  âœ… Get Git repo access
  âœ… Look at Airflow â€” find the tcpl_cpg_full_pipeline DAG
  âœ… Watch one full pipeline run (green tasks = success)

DAY 2 â€” EXPLORE THE DATA
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… Open Snowflake â†’ TCPL_CPG_DB
  âœ… SELECT * FROM DW.FACT_SALES LIMIT 100
  âœ… SELECT * FROM ML_OUTPUTS.DEMAND_FORECAST_PREDICTIONS LIMIT 50
  âœ… Look at DIM tables â€” understand SKU master, region master
  âœ… Run the sales performance mart view query

DAY 3 â€” UNDERSTAND THE CODE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… Open git repo â†’ read dags/tcpl_cpg_full_pipeline_dag.py
  âœ… Read glue/sap_sales_transform.py â€” understand each step
  âœ… Look at lambdas/data_quality/handler.py
  âœ… Find a recent Glue job in AWS Console â†’ read its CloudWatch logs

DAY 4 â€” SHADOW A PIPELINE RUN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… Stay online during 2AM run (or check logs next morning)
  âœ… Open Airflow â†’ see all tasks go green one by one
  âœ… Check CloudWatch â†’ see DQ metrics update
  âœ… Check Snowflake â†’ verify new rows loaded for today's date

DAY 5 â€” FIRST SMALL TASK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Common first tasks assigned:
  - Add a new column to Glue ETL script
  - Write a new Snowflake view for a business request
  - Update a CloudWatch alarm threshold
  - Add a new DAG task for a new data source
```

---

## ğŸ§© QUICK REFERENCE CARD

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMPONENT   â”‚  WHERE TO FIND IT                â”‚  WHAT YOU DO THERE       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Airflow      â”‚ MWAA â†’ Airflow UI URL             â”‚ Monitor, re-trigger DAGs â”‚
â”‚ Glue         â”‚ AWS Console â†’ Glue â†’ Jobs         â”‚ Run/debug ETL jobs       â”‚
â”‚ Lambda       â”‚ AWS Console â†’ Lambda â†’ Functions  â”‚ Edit, test functions     â”‚
â”‚ SageMaker    â”‚ AWS Console â†’ SageMaker â†’ Studio  â”‚ Monitor pipelines        â”‚
â”‚ S3           â”‚ AWS Console â†’ S3 â†’ tcpl-datalake  â”‚ Check files landed       â”‚
â”‚ Snowflake    â”‚ Snowflake URL (SnowSight UI)       â”‚ SQL queries, data check  â”‚
â”‚ CloudWatch   â”‚ AWS Console â†’ CloudWatch          â”‚ Logs, metrics, alarms    â”‚
â”‚ Secrets Mgr  â”‚ AWS Console â†’ Secrets Manager     â”‚ Credentials (read-only)  â”‚
â”‚ Git          â”‚ GitHub / CodeCommit repo           â”‚ All code changes         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ REGION       â”‚ AWS ap-south-1 (Mumbai)            â”‚ Always check region!     â”‚
â”‚ SCHEDULE     â”‚ Pipeline: 2AM IST daily            â”‚                          â”‚
â”‚ ALERTS       â”‚ Slack: #tcpl-data-alerts           â”‚                          â”‚
â”‚ CONTACT      â”‚ TCPL SAP team for file issues      â”‚                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

*You've got this bro ğŸ’ª â€” The stack is AWS Glue + MWAA + Lambda + SageMaker + Snowflake.
Airflow is the brain. Glue is the factory. Lambda is the helper. SageMaker is the ML lab. Snowflake is the warehouse.*

*Read the Airflow DAG first on Day 1 â€” that tells you the ENTIRE pipeline story in one file.*
